"""
Lambda 2: Rewrite Worker
Performs the actual rewrite operation asynchronously and stores the result in S3.
This is invoked by Lambda 1 and does not return a response to API Gateway.
"""

import json
import boto3
import os
import logging
import hashlib
import re
from datetime import datetime
from typing import Dict, Tuple, Any, Optional, List, Set

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS clients
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-east-1")
s3_client = boto3.client("s3")

# Configuration
BUCKET_NAME = os.environ.get("BUCKET_NAME", "vision-investigation-system-052904446370")
MODEL_ID = "amazon.nova-lite-v1:0"

# ===== Size & performance limits =====
MAX_TOTAL_CHARS = 60000  # Max chars for entire document
CHUNK_SIZE = 15000  # Characters per chunk
OVERLAP_SIZE = 500  # Overlap between chunks for context
MAX_TOKENS = 4000  # Bedrock output limit per chunk


def get_safe_log_info(text: str, session_id: Optional[str] = None) -> Dict[str, Any]:
    """Return safe hashed log info without exposing private data."""
    return {
        "text_hash": hashlib.md5(text.encode("utf-8")).hexdigest()[:8],
        "text_length": len(text),
        "session_id": session_id or "unknown"
    }


def read_text_from_s3(bucket: str, key: str) -> str:
    """Read text file from S3 bucket."""
    try:
        obj = s3_client.get_object(Bucket=bucket, Key=key)
        return obj["Body"].read().decode("utf-8")
    except Exception as e:
        logger.error(f"‚ùå Failed to read s3://{bucket}/{key} - {e}")
        raise


def build_rewrite_prompts(original_text: str) -> Tuple[str, str]:
    system = (
        "ÿ£ŸÜÿ™ ŸÖÿ≠ÿ±ÿ± ÿ™ŸÇÿßÿ±Ÿäÿ± ÿ¨ŸÜÿßÿ¶Ÿäÿ© ŸÑŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ©. ŸÖŸáŸÖÿ™ŸÉ ÿ™ŸÜÿ∏ŸäŸÖ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ŸÅŸÇÿ∑ ÿØŸàŸÜ ÿ•ÿ∂ÿßŸÅÿ© "
        "ÿ£Ÿà ÿ≠ÿ∞ŸÅ ÿ£Ÿä ŸÖÿπŸÑŸàŸÖÿ© ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä.\n\n"
        "ŸÇŸàÿßÿπÿØ ÿ•ŸÑÿ≤ÿßŸÖŸäÿ©:\n"
        "- ŸÑÿß ÿ™ÿ∂ŸÅ ÿ£Ÿä ÿ£ÿ≥ŸÖÿßÿ° ÿ£Ÿà ÿ£ÿ±ŸÇÿßŸÖ ÿ£Ÿà ŸÖŸàÿßŸÇÿπ ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµ.\n"
        "- ŸÑÿß ÿ™ÿ≠ÿ∞ŸÅ ÿ£Ÿä ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ¨ŸàŸáÿ±Ÿäÿ© (ÿ£ÿ≥ŸÖÿßÿ°ÿå ÿ™Ÿàÿßÿ±ŸäÿÆÿå ÿ£ŸÖÿßŸÉŸÜÿå ÿ£ŸÇŸàÿßŸÑÿå ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™).\n"
        "- ÿ•ÿ∞ÿß ŸàŸèÿ¨ÿØÿ™ ÿßŸÑŸÅŸÇÿ±ÿ© ŸÜŸÅÿ≥Ÿáÿß ŸÖŸÉÿ±ÿ±ÿ©ÿå ÿßÿ≠ÿ™ŸÅÿ∏ ÿ®ŸÜÿ≥ÿÆÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑.\n"
        "- ÿ•ÿ∞ÿß ŸàŸèÿ¨ÿØ ÿπŸÜŸàÿßŸÜ ÿ±ÿ≥ŸÖŸä ŸÖŸÉÿ±ÿ± (ŸÖÿ´ŸÑ ŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜ ‚Äì ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ©)ÿå ÿßÿ≠ÿ™ŸÅÿ∏ ÿ®ÿ£ŸàŸÑ ÿ™ŸÉÿ±ÿßÿ± ŸÅŸÇÿ∑.\n"
        "- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ÿ¨ŸÖŸÑ ŸÖŸÉÿ≥Ÿàÿ±ÿ© ÿ£Ÿà ŸÉŸÑŸÖÿßÿ™ ÿ∫Ÿäÿ± ŸÖŸÅŸáŸàŸÖÿ© ŸÜÿ™Ÿäÿ¨ÿ© OCRÿå ÿßÿ≠ÿ∞ŸÅŸáÿß ÿØŸàŸÜ ŸÖÿ≠ÿßŸàŸÑÿ© ÿ™ÿÆŸÖŸäŸÜ ŸÖÿπŸÜÿßŸáÿß.\n"
        "- ÿ•ÿ∞ÿß ÿßÿ≠ÿ™ŸàŸâ ÿßŸÑŸÜÿµ ÿπŸÑŸâ ŸÅÿ™ÿ≠/ÿ•ÿ∫ŸÑÿßŸÇ ÿßŸÑŸÖÿ≠ÿ∂ÿ± ÿπÿØÿ© ŸÖÿ±ÿßÿ™ÿå ŸÇŸÖ ÿ®ÿ™ÿ±ÿ™Ÿäÿ®Ÿáÿß ÿ≤ŸÖŸÜŸäŸãÿß ŸàÿØŸÖÿ¨Ÿáÿß ÿØŸàŸÜ ÿ™ŸÉÿ±ÿßÿ±.\n"
        "- ÿ•ÿ∞ÿß ÿ∏Ÿáÿ± ÿ™ŸÇÿ±Ÿäÿ± ÿ¢ÿÆÿ± ÿØÿßÿÆŸÑ ŸÜŸÅÿ≥ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿå ÿ∂ÿπ Ÿáÿ∞Ÿá ÿßŸÑÿ£ÿ¨ÿ≤ÿßÿ° ŸÅŸä ŸÇÿ≥ŸÖ (ŸÖŸÑÿßÿ≠ŸÇ ÿ•ÿ∂ÿßŸÅŸäÿ©) ÿØŸàŸÜ ÿØŸÖÿ¨Ÿáÿß ŸÖÿπ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä.\n"
        "- ŸÑÿß ÿ™ŸÉÿ™ÿ® ÿßŸÑŸÜÿµ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖÿ±ÿ©.\n"
        "- ŸÑÿß ÿ™ÿπŸäÿØ ÿµŸäÿßÿ∫ÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ŸÜŸÅÿ≥Ÿá ŸÖÿ±ÿ™ŸäŸÜ.\n"
        "- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© ŸÅŸÇÿ∑.\n\n"
        "ŸÇŸàÿßÿπÿØ ÿßŸÑÿ¨ÿØÿßŸàŸÑ ŸàÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:\n"
        "- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ¨ÿØÿßŸàŸÑ markdown ŸÅŸÇÿ∑ ŸÑŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑŸÖÿ≠ÿØÿØÿ©: ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅÿå ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ±ÿå ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸáŸÖÿ©.\n"
        "- ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ÿ£Ÿä ÿ¨ÿØÿßŸàŸÑ ŸÖŸÜ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä (ŸÖÿ´ŸÑ: ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑÿ®ŸÑÿßÿ∫ÿå ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ŸàÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ÿå ÿ¨ÿØÿßŸàŸÑ OCR).\n"
        "- ÿßÿ≠ÿ∞ŸÅ ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿßŸÑŸÅÿßÿ±ÿ∫ÿ© ÿ£Ÿà ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ•ÿØÿßÿ±Ÿäÿ© ŸÖŸÉÿ±ÿ±ÿ©.\n"
        "- ÿßŸÑÿ£ŸÇŸàÿßŸÑ ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ™ŸèŸÉÿ™ÿ® ŸÉŸÅŸÇÿ±ÿßÿ™ ŸÜÿµŸäÿ©ÿå ŸÑŸäÿ≥ ŸÉÿ¨ÿØÿßŸàŸÑ.\n"
        "- ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ•ÿØÿßÿ±Ÿäÿ© (ŸÖÿ´ŸÑ: ŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜÿå ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ©ÿå ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫) ÿ™ŸèŸÉÿ™ÿ® ŸÅŸä ŸÇÿ≥ŸÖ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÇÿ∂Ÿäÿ© ŸÅŸÇÿ∑.\n"
        "- ŸÑÿß ÿ™ŸÉÿ™ÿ® '####' ÿ£Ÿà '###' ÿ£Ÿà '#' ÿ£Ÿà 'ÿßŸÑ-' ŸÅŸä ÿ£Ÿä ŸÖŸÉÿßŸÜ.\n"
        "- ÿßÿ≥ÿ™ÿÆÿØŸÖ '##' ŸÅŸÇÿ∑ ŸÑŸÑÿπŸÜÿßŸàŸäŸÜ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ÿßŸÑŸÖÿ≠ÿØÿØÿ© ŸÅŸä ÿßŸÑŸáŸäŸÉŸÑ.\n"
        "- ŸÑÿß ÿ™ŸÉÿ™ÿ® ÿπŸÜÿßŸàŸäŸÜ ŸÅÿ±ÿπŸäÿ© ÿ®ÿµŸäÿ∫ÿ© '#### ÿßŸÑ-' ÿ£Ÿà ÿ£Ÿä ÿµŸäÿ∫ÿ© ŸÖÿ¥ÿßÿ®Ÿáÿ©.\n"
    )

    user = (
        "ÿ•ŸÑŸäŸÉ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä ŸÑŸÑÿ™ŸÇÿ±Ÿäÿ± ŸÉŸÖÿß Ÿàÿ±ÿØ:\n\n"
        f"{original_text}\n\n"
        "ÿßŸÑŸÖÿ∑ŸÑŸàÿ®: ÿ•ÿπÿßÿØÿ© ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ÿ®ÿµŸäÿ∫ÿ© ŸÖŸÜÿ∏ŸÖÿ© ŸàŸÅŸÇ ÿßŸÑŸáŸäŸÉŸÑ ÿßŸÑÿ™ÿßŸÑŸä **ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑**ÿå "
        "ÿπŸÑŸâ ÿ£ŸÜ ÿ™ŸèŸÖŸÑÿ£ ÿßŸÑÿ≠ŸÇŸàŸÑ ŸÖŸÜ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä ÿØŸàŸÜ ÿ£Ÿä ÿ•ÿ∂ÿßŸÅÿ© ÿ£Ÿà ÿßÿÆÿ™ÿ±ÿßÿπ ŸÖÿπŸÑŸàŸÖÿßÿ™.\n\n"
        "**ŸáŸäŸÉŸÑ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑŸÜŸáÿßÿ¶Ÿä (ÿßÿ™ÿ®ÿπŸá ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ŸàŸÑÿß ÿ™ŸÉÿ±ÿ± ÿ£Ÿä ŸÇÿ≥ŸÖ):**\n\n"
        "## ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÇÿ∂Ÿäÿ©\n\n"
        "- ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫ / ÿ±ŸÇŸÖ ÿßŸÑŸÇÿ∂Ÿäÿ©:\n"
        "- ŸÜŸàÿπ ÿßŸÑŸÇÿ∂Ÿäÿ© (ÿ•ŸÜ Ÿàÿ¨ÿØ):\n"
        "- ÿßŸÑÿ¨Ÿáÿ© (ŸÖÿ´ÿßŸÑ: ŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßÿµŸÖÿ©ÿå ŸÖÿ±ŸÉÿ≤ ÿ¥ÿ±ÿ∑ÿ© ÿßŸÑÿ≠Ÿàÿ±ÿ©):\n"
        "- ÿ™ÿßÿ±ŸäÿÆ ŸàŸàŸÇÿ™ ŸÅÿ™ÿ≠ ÿßŸÑŸÖÿ≠ÿ∂ÿ±:\n\n"
        "## ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ\n\n"
        "**Ÿäÿ¨ÿ® ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ ŸÅŸä ÿ¨ÿØŸàŸÑ markdown ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ¥ŸÉŸÑ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑:**\n\n"
        "| ÿßŸÑÿµŸÅÿ© | ÿßŸÑÿßÿ≥ŸÖ ÿßŸÑŸÉÿßŸÖŸÑ | ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ© | ÿßŸÑÿ±ŸÇŸÖ ÿßŸÑÿ¥ÿÆÿµŸä | ÿßŸÑŸáÿßÿ™ŸÅ |\n"
        "| --- | --- | --- | --- | --- |\n"
        "| (ÿßŸÑÿµŸÅÿ©) | (ÿßŸÑÿßÿ≥ŸÖ ŸÉŸÖÿß Ÿàÿ±ÿØ) | (ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©) | (ÿßŸÑÿ±ŸÇŸÖ) | (ÿ±ŸÇŸÖ ÿßŸÑŸáÿßÿ™ŸÅ) |\n\n"
        "ŸÖŸÑÿßÿ≠ÿ∏ÿ©: ÿßŸÉÿ™ÿ® ÿ≥ÿ∑ÿ± ŸÑŸÉŸÑ ÿ¥ÿÆÿµ ŸÖÿ∞ŸÉŸàÿ± ŸÅŸä ÿßŸÑŸÜÿµ. ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÖÿπŸÑŸàŸÖÿ© ŸÜÿßŸÇÿµÿ©ÿå ÿßŸÉÿ™ÿ® 'ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±'.\n"
        "ÿßŸÑÿµŸÅÿßÿ™ ÿßŸÑŸÖÿ≠ÿ™ŸÖŸÑÿ©: ŸÖÿ®ŸÑÿ∫ÿå ŸÖÿØÿπŸâ ÿπŸÑŸäŸáÿå ÿ¥ÿßŸáÿØÿå ÿ∂ÿßÿ®ÿ∑ÿå ŸÖÿ≠ÿ±ÿ± ŸÖÿ≠ÿ∂ÿ±ÿå ŸÖÿ™ÿ±ÿ¨ŸÖÿå ŸàŸÉŸäŸÑ ŸÜŸäÿßÿ®ÿ©ÿå ÿ•ŸÑÿÆ.\n\n"
        "## ŸÖŸÑÿÆÿµ ÿßŸÑÿ≠ÿßÿØÿ´\n"
        "- ÿßŸÑÿ≤ŸÖÿßŸÜÿå ÿßŸÑŸÖŸÉÿßŸÜÿå ŸàÿµŸÅ ÿßŸÑŸÅÿπŸÑ ÿßŸÑÿ•ÿ¨ÿ±ÿßŸÖŸä.\n\n"
        "## ŸÖÿ≥ÿ±ÿ≠ ÿßŸÑÿ≠ÿßÿØÿ´\n"
        "- ÿßŸÑÿπŸÜŸàÿßŸÜ ÿßŸÑŸÉÿßŸÖŸÑ ŸÉŸÖÿß Ÿàÿ±ÿØ.\n\n"
        "## ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™\n\n"
        "ÿ•ÿ∞ÿß Ÿàÿ¨ÿØÿ™ ŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÉÿ™ÿ®Ÿáÿß ŸÅŸä ÿ¨ÿØŸàŸÑ:\n\n"
        "| ŸÖ | ÿßŸÑÿµŸÜŸÅ | ÿßŸÑŸàÿµŸÅ | ÿßŸÑÿ≠ÿßŸÑÿ© | ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ |\n"
        "| --- | --- | --- | --- | --- |\n"
        "| 1 | (ÿßŸÑÿµŸÜŸÅ) | (ÿßŸÑŸàÿµŸÅ ÿßŸÑÿ™ŸÅÿµŸäŸÑŸä) | (ÿßŸÑÿ≠ÿßŸÑÿ©) | (ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™) |\n\n"
        "ŸÖÿ´ÿßŸÑ:\n"
        "| ŸÖ | ÿßŸÑÿµŸÜŸÅ | ÿßŸÑŸàÿµŸÅ | ÿßŸÑÿ≠ÿßŸÑÿ© | ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ |\n"
        "| --- | --- | --- | --- | --- |\n"
        "| 1 | ŸÖŸÜÿ¥ÿßÿ± ŸÉŸáÿ±ÿ®ÿßÿ¶Ÿä | ŸÖŸÜÿ¥ÿßÿ± ŸäÿØŸàŸä | ŸÖÿ≥ÿ™ÿπŸÖŸÑ | ŸÖÿ∂ÿ®Ÿàÿ∑ ŸÅŸä ÿßŸÑŸÖŸàŸÇÿπ |\n"
        "| 2 | ÿ£ÿ≥ŸÑÿßŸÉ ŸÉŸáÿ±ÿ®ÿßÿ¶Ÿäÿ© | ÿ≠ÿ≤ŸÖÿ© ÿ£ÿ≥ŸÑÿßŸÉ ŸÖÿ™ŸÜŸàÿπÿ© | ÿ¨ÿØŸäÿØÿ© | ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± |\n\n"
        "## ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ±\n\n"
        "ÿ•ÿ∞ÿß Ÿàÿ¨ÿØÿ™ ÿ£ÿ∂ÿ±ÿßÿ±ÿå ÿßŸÉÿ™ÿ®Ÿáÿß ŸÅŸä ÿ¨ÿØŸàŸÑ:\n\n"
        "| ŸÖ | ÿßŸÑŸÖŸàŸÇÿπ/ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑŸÖÿ™ÿ∂ÿ±ÿ± | ŸàÿµŸÅ ÿßŸÑÿ∂ÿ±ÿ± | ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ™ŸÇÿØŸäÿ±Ÿäÿ© |\n"
        "| --- | --- | --- | --- |\n"
        "| 1 | (ÿßŸÑŸÖŸàŸÇÿπ) | (ŸàÿµŸÅ ÿßŸÑÿ™ŸÑŸÅ) | (ÿßŸÑŸÖÿ®ŸÑÿ∫ ÿ•ŸÜ Ÿàÿ¨ÿØ) |\n\n"
        "ŸÖÿ´ÿßŸÑ:\n"
        "| ŸÖ | ÿßŸÑŸÖŸàŸÇÿπ/ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑŸÖÿ™ÿ∂ÿ±ÿ± | ŸàÿµŸÅ ÿßŸÑÿ∂ÿ±ÿ± | ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ™ŸÇÿØŸäÿ±Ÿäÿ© |\n"
        "| --- | --- | --- | --- |\n"
        "| 1 | ÿ∫ÿ±ŸÅÿ© ÿßŸÑŸÖÿµÿπÿØ | ŸÉÿ≥ÿ± ŸÅŸä ÿßŸÑÿ≤ÿ¨ÿßÿ¨ ÿßŸÑÿ£ŸÖÿßŸÖŸä | 500 ÿØŸäŸÜÿßÿ± |\n"
        "| 2 | ÿßŸÑÿ¥ŸÇÿ© ÿ±ŸÇŸÖ 12 | ÿ™ŸÑŸÅ ŸÅŸä ÿßŸÑÿ®ÿßÿ® ÿßŸÑÿÆÿ¥ÿ®Ÿä | ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± |\n\n"
        "## ÿßŸÑÿ£ŸÇŸàÿßŸÑ\n"
        "ÿßŸÉÿ™ÿ® ÿ£ŸÇŸàÿßŸÑ ŸÉŸÑ ÿ¥ÿÆÿµ ŸÅŸä ŸÅŸÇÿ±ÿßÿ™ ŸÖŸÜŸÅÿµŸÑÿ©:\n\n"
        "**ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿ®ŸÑÿ∫:**\n\n"
        "(ŸÑÿÆÿµ ŸÖÿß ŸÇÿßŸÑŸá ÿßŸÑŸÖÿ®ŸÑÿ∫ ŸÖŸÜ ÿßŸÑŸÜÿµ)\n\n"
        "**ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá:**\n\n"
        "(ŸÑÿÆÿµ ŸÖÿß ŸÇÿßŸÑŸá ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá ŸÖŸÜ ÿßŸÑŸÜÿµ)\n\n"
        "**ÿ£ŸÇŸàÿßŸÑ ÿßŸÑÿ¥ŸáŸàÿØ:**\n\n"
        "(ŸÑÿÆÿµ ÿ£ŸÇŸàÿßŸÑ ŸÉŸÑ ÿ¥ÿßŸáÿØ)\n\n"
        "## ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿ¥ÿ±ÿ∑ÿ©\n"
        "- ŸÉŸÑ ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿ®ÿ≠ÿ´ ŸàÿßŸÑÿ™ÿ≠ÿ±Ÿä ŸàÿßŸÑŸÖÿπÿßŸäŸÜÿ© ŸÉŸÖÿß Ÿàÿ±ÿØÿ™.\n"
        "- ÿßŸÑÿ™Ÿàÿ¨Ÿá ŸÑŸÑŸÖŸàŸÇÿπÿå ÿßŸÑŸÖÿπÿßŸäŸÜÿ©ÿå ÿßŸÑÿ™ÿµŸàŸäÿ±ÿå ÿßŸÑÿ≠ÿ¨ÿ≤ÿå ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑŸÖŸäÿØÿßŸÜŸä.\n\n"
        "## ÿßŸÑÿ™ŸÜÿßÿ≤ŸÑ ÿ£Ÿà ÿßŸÑÿµŸÑÿ≠\n"
        "- ÿ•ÿ∞ÿß ÿ™ŸÜÿßÿ≤ŸÑ ÿßŸÑŸÖÿ®ŸÑÿ∫ ÿπŸÜ ÿßŸÑÿ®ŸÑÿßÿ∫ÿå ÿßÿ∞ŸÉÿ± ÿßŸÑÿ™ÿßÿ±ŸäÿÆ ŸàÿßŸÑÿ≥ÿ®ÿ® ŸàÿßŸÑÿ™ŸÅÿßÿµŸäŸÑ ŸÉŸÖÿß Ÿàÿ±ÿØÿ™.\n"
        "- ÿ•ÿ∞ÿß ÿ™ŸÖ ÿßŸÑÿµŸÑÿ≠ ÿ®ŸäŸÜ ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅÿå ÿßÿ∞ŸÉÿ± ÿ∞ŸÑŸÉ.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿ≠ÿØÿ´ ÿ™ŸÜÿßÿ≤ŸÑ ÿ£Ÿà ÿµŸÑÿ≠ÿå ÿßŸÉÿ™ÿ®: ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±.\n\n"
        "## ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ŸàŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑŸÜŸäÿßÿ®ÿ©\n"
        "ÿßÿ∞ŸÉÿ± ÿ¨ŸÖŸäÿπ ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™ ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ®ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ:\n"
        "- ŸÇÿ±ÿßÿ±ÿßÿ™ ŸàŸÉŸäŸÑ ÿßŸÑŸÜŸäÿßÿ®ÿ© (ÿßŸÑÿ≠ÿ¨ÿ≤ÿå ÿßŸÑÿ•ŸÅÿ±ÿßÿ¨ÿå ÿ™ÿ≥ŸÑŸäŸÖ ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÑÿ•ÿ≠ÿßŸÑÿ©ÿå ÿßŸÑÿ≠ŸÅÿ∏...).\n"
        "- ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÉÿßŸÖŸÑÿ© ŸÑŸÉŸÑ ŸÇÿ±ÿßÿ±.\n"
        "- ÿ£ÿ≥ŸÖÿßÿ° ŸàŸÉŸÑÿßÿ° ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿ∞ŸäŸÜ ÿßÿ™ÿÆÿ∞Ÿàÿß ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™.\n\n"
        "## ÿ™ÿ≥ŸÑŸäŸÖ ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™\n"
        "- ÿ•ÿ∞ÿß ÿ™ŸÖ ÿ™ÿ≥ŸÑŸäŸÖ ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ ŸÑÿ£ÿ≠ÿØÿå ÿßÿ∞ŸÉÿ±: ŸÖŸÜ ÿßÿ≥ÿ™ŸÑŸÖÿå ŸÖÿ™Ÿâÿå ŸÖÿß ŸáŸä ÿßŸÑÿ£ÿ¥Ÿäÿßÿ°.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿ™ÿ≥ŸÑŸäŸÖÿå ÿßÿ∞ŸÉÿ± ÿßŸÑÿ≥ÿ®ÿ®.\n\n"
        "## ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸáŸÖÿ©\n\n"
        "**Ÿäÿ¨ÿ® ŸÉÿ™ÿßÿ®ÿ© ÿ¨ŸÖŸäÿπ ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸÅŸä ÿ¨ÿØŸàŸÑ markdown:**\n\n"
        "| ÿßŸÑÿ™ÿßÿ±ŸäÿÆ ŸàÿßŸÑŸàŸÇÿ™ | ÿßŸÑÿ≠ÿØÿ´ | ÿßŸÑÿ¨Ÿáÿ©/ÿßŸÑŸÖÿ≥ÿ§ŸàŸÑ |\n"
        "| --- | --- | --- |\n"
        "| (ÿßŸÑÿ™ÿßÿ±ŸäÿÆ ŸàÿßŸÑŸàŸÇÿ™) | (ŸàÿµŸÅ ÿßŸÑÿ≠ÿØÿ´) | (ÿßŸÑÿ¨Ÿáÿ© ÿ£Ÿà ÿßŸÑÿ¥ÿÆÿµ) |\n\n"
        "ÿßŸÉÿ™ÿ® ÿ¨ŸÖŸäÿπ ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ© ŸÅŸä ÿßŸÑŸÜÿµ ÿ®ÿ™ÿ±ÿ™Ÿäÿ® ÿ≤ŸÖŸÜŸä ŸÖŸÜ ÿßŸÑÿ£ŸÇÿØŸÖ ÿ•ŸÑŸâ ÿßŸÑÿ£ÿ≠ÿØÿ´.\n\n"
        "## ÿßŸÑÿ™ŸàŸÇŸäÿπÿßÿ™ ŸàÿßŸÑŸÖÿ≠ÿ±ÿ±ŸäŸÜ\n\n"
        "ÿßÿ∞ŸÉÿ± ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸàÿßŸÑÿ±ÿ™ÿ® ŸàÿßŸÑÿ™ŸàŸÇŸäÿπÿßÿ™ ŸÉŸÖÿß Ÿàÿ±ÿØÿ™ ŸÅŸä ÿßŸÑŸÜÿµ:\n\n"
        "**ŸÖÿ≠ÿ±ÿ±Ÿä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±:**\n"
        "- (ÿßŸÑÿ±ÿ™ÿ®ÿ© ŸàÿßŸÑÿßÿ≥ŸÖÿå ÿßŸÑÿ™ÿßÿ±ŸäÿÆ)\n\n"
        "**ÿßŸÑÿ∂ÿ®ÿßÿ∑ ÿßŸÑŸÖÿ¥ÿ±ŸÅŸäŸÜ:**\n"
        "- (ÿßŸÑÿ±ÿ™ÿ®ÿ© ŸàÿßŸÑÿßÿ≥ŸÖ)\n\n"
        "**ŸàŸÉŸÑÿßÿ° ÿßŸÑŸÜŸäÿßÿ®ÿ©:**\n"
        "- (ÿßŸÑÿßÿ≥ŸÖ ŸàÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™)\n\n"
        "**ÿ£ÿÆÿµÿßÿ¶ŸäŸä ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ:**\n"
        "- (ÿßŸÑÿßÿ≥ŸÖ)\n\n"
        "## ŸÖŸÑÿßÿ≠ŸÇ ÿ•ÿ∂ÿßŸÅŸäÿ© (ÿ•ŸÜ Ÿàÿ¨ÿØÿ™)\n\n"
        "**ŸÖŸÑÿßÿ≠ÿ∏ÿ©:** Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ ŸÑŸÑÿ™ŸÇÿßÿ±Ÿäÿ± ÿßŸÑŸÖŸÜŸÅÿµŸÑÿ© ÿ£Ÿà ÿßŸÑŸÖŸÑÿ≠ŸÇÿ© ŸÅŸÇÿ∑.\n"
        "- ŸÑÿß ÿ™ÿ∂ÿπ ŸáŸÜÿß ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿπÿßÿØŸäÿ© ÿ£Ÿà ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™.\n"
        "- ŸÑÿß ÿ™ŸÉÿ±ÿ± ÿßŸÑÿ¨ÿØÿßŸàŸÑ (ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅÿå ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ±ÿå ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ) ŸáŸÜÿß.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸÉŸÜ ŸáŸÜÿßŸÉ ÿ™ŸÇÿ±Ÿäÿ± ŸÖŸÜŸÅÿµŸÑ ÿ£Ÿà ŸÖŸÑÿ≠ŸÇÿå ÿßÿ≠ÿ∞ŸÅ Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ ÿ™ŸÖÿßŸÖÿßŸã.\n\n"
        "---\n\n"
        "**ÿ™ÿπŸÑŸäŸÖÿßÿ™ ŸÖŸáŸÖÿ© ÿ¨ÿØÿßŸã:**\n\n"
        "**ŸÖÿß Ÿäÿ¨ÿ® ŸÉÿ™ÿßÿ®ÿ™Ÿá:**\n"
        "1. ÿßŸÇÿ±ÿ£ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä ŸÉÿßŸÖŸÑÿßŸã ŸÖŸÜ ÿßŸÑÿ®ÿØÿßŸäÿ© ÿ•ŸÑŸâ ÿßŸÑŸÜŸáÿßŸäÿ©.\n"
        "2. ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ¨ŸàŸáÿ±Ÿäÿ© ŸÅŸÇÿ∑ Ÿàÿ∂ÿπŸáÿß ŸÅŸä ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑŸÖÿ≠ÿØÿØÿ© ÿ£ÿπŸÑÿßŸá.\n"
        "3. ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ© ŸÅŸÇÿ∑: ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅÿå ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ±ÿå ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸáŸÖÿ©.\n"
        "4. ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ŸÇŸàÿßŸÑ ÿ™ŸèŸÉÿ™ÿ® ŸÉŸÅŸÇÿ±ÿßÿ™ ŸÜÿµŸäÿ© ŸÖŸÜÿ∏ŸÖÿ©ÿå ŸÑŸäÿ≥ ŸÉÿ¨ÿØÿßŸàŸÑ.\n"
        "5. ÿ¨ŸÖŸäÿπ ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ™ŸèŸÉÿ™ÿ® ŸÉŸÅŸÇÿ±ÿßÿ™ ÿ£Ÿà ŸÜŸÇÿßÿ∑ÿå ŸÑŸäÿ≥ ŸÉÿ¨ÿØÿßŸàŸÑ.\n\n"
        "**ŸÖÿß Ÿäÿ¨ÿ® ÿ≠ÿ∞ŸÅŸá ŸàÿπÿØŸÖ ŸÜÿ≥ÿÆŸá:**\n"
        "1. ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ÿ¨ÿØÿßŸàŸÑ OCR ŸÖŸÜ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä (ŸÖÿ´ŸÑ: ÿ¨ÿØŸàŸÑ 'ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑÿ®ŸÑÿßÿ∫' ÿßŸÑŸÖŸÉÿ±ÿ±).\n"
        "2. ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ÿ¨ÿØÿßŸàŸÑ 'ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ŸàÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™' ŸÖŸÜ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä.\n"
        "3. ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ•ÿØÿßÿ±Ÿäÿ© ŸÖŸÉÿ±ÿ±ÿ© ŸÅŸä ÿ¨ÿØÿßŸàŸÑ.\n"
        "4. ŸÑÿß ÿ™ŸÉÿ™ÿ® ÿ¨ÿØÿßŸàŸÑ ŸÅÿßÿ±ÿ∫ÿ© ÿ£Ÿà ÿ∫Ÿäÿ± ŸÖŸÅŸäÿØÿ©.\n"
        "5. ŸÑÿß ÿ™ŸÉÿ±ÿ± ÿßŸÑÿπŸÜÿßŸàŸäŸÜ ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© (ŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜÿå ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ©) ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖÿ±ÿ©.\n"
        "6. ŸÑÿß ÿ™ŸÉÿ±ÿ± ÿ£Ÿä ŸÇÿ≥ŸÖ ŸÖŸÜ ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ© ÿ£ÿπŸÑÿßŸá (ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÇÿ∂Ÿäÿ© ÿ™ŸèŸÉÿ™ÿ® ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑).\n"
        "7. ŸÑÿß ÿ™ŸÉÿ™ÿ® 'ŸÖŸÑÿßÿ≠ŸÇ ÿ•ÿ∂ÿßŸÅŸäÿ©' ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸÉŸÜ ŸáŸÜÿßŸÉ ÿ™ŸÇÿ±Ÿäÿ± ŸÖŸÜŸÅÿµŸÑ.\n"
        "8. ŸÑÿß ÿ™ÿ∂ÿπ ÿ¨ÿØÿßŸàŸÑ ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ/ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™/ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ±/ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸÅŸä ŸÇÿ≥ŸÖ 'ŸÖŸÑÿßÿ≠ŸÇ ÿ•ÿ∂ÿßŸÅŸäÿ©'.\n"
        "9. **ŸÖŸÖŸÜŸàÿπ ÿ™ŸÖÿßŸÖÿßŸã:** ŸÑÿß ÿ™ŸÉÿ™ÿ® '#### ÿßŸÑ-' ÿ£Ÿà '### ÿßŸÑ-' ÿ£Ÿà ÿ£Ÿä ÿπŸÜŸàÿßŸÜ Ÿäÿ®ÿØÿ£ ÿ®ŸÄ 'ÿßŸÑ-'.\n"
        "10. **ŸÖŸÖŸÜŸàÿπ ÿ™ŸÖÿßŸÖÿßŸã:** ŸÑÿß ÿ™ŸÉÿ™ÿ® ŸÅŸÇÿ±ÿßÿ™ ŸÖÿ™ÿπÿØÿØÿ© ÿ™ÿ®ÿØÿ£ ŸÉŸÑ ŸÖŸÜŸáÿß ÿ®ŸÄ '#### ÿßŸÑ-' ŸÖÿ™ÿ®Ÿàÿπÿ© ÿ®ŸÖÿπŸÑŸàŸÖÿßÿ™.\n"
        "11. ŸÑÿß ÿ™ŸÉÿ™ÿ® ŸÇŸàÿßÿ¶ŸÖ ÿ∑ŸàŸäŸÑÿ© ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ®ÿµŸäÿ∫ÿ© '#### ÿßŸÑ- ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ©'.\n\n"
        "**ŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿßŸÉÿ™ŸÖÿßŸÑ:**\n"
        "1. ŸÑÿß ÿ™ÿ™ŸàŸÇŸÅ ÿπŸÜ ÿßŸÑŸÉÿ™ÿßÿ®ÿ© ÿ≠ÿ™Ÿâ ÿ™ŸÜÿ™ŸáŸä ŸÖŸÜ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ.\n"
        "2. ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ŸÉÿ™ÿßÿ®ÿ© ÿ¨ŸÖŸäÿπ ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™ ŸàÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ≠ÿ™Ÿâ ÿ¢ÿÆÿ± ÿ≥ÿ∑ÿ±.\n"
        "3. ÿ•ÿ∞ÿß Ÿàÿ¨ÿØÿ™ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿπŸÜ ÿßŸÑÿ™ŸÜÿßÿ≤ŸÑ ÿ£Ÿà ÿßŸÑÿµŸÑÿ≠ ÿ£Ÿà ÿ™ÿ≥ŸÑŸäŸÖ ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå Ÿäÿ¨ÿ® ŸÉÿ™ÿßÿ®ÿ™Ÿáÿß.\n"
        "4. ÿ¨ÿØŸàŸÑ ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸÖŸÜ ÿ£ŸàŸÑ ÿßŸÑÿ≠ÿßÿØÿ´ ÿ≠ÿ™Ÿâ ÿ¢ÿÆÿ± ÿ•ÿ¨ÿ±ÿßÿ°.\n"
        "5. ŸÑÿß ÿ™ÿ™ÿ±ŸÉ ÿ£Ÿä ŸÇÿ≥ŸÖ ŸÅÿßÿ±ÿ∫ÿßŸã ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµ.\n"
        "6. ÿßŸÉÿ™ÿ® ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± **ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑** ÿ®ÿ¥ŸÉŸÑ ŸÉÿßŸÖŸÑ Ÿàÿßÿ≠ÿ™ÿ±ÿßŸÅŸä - ÿßÿ™ÿ®ÿπ ÿßŸÑŸáŸäŸÉŸÑ ÿßŸÑŸÖÿ≠ÿØÿØ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑.\n"
        "7. ŸÉŸÑ ŸÇÿ≥ŸÖ ŸäŸèŸÉÿ™ÿ® ŸÅŸä ŸÖŸÉÿßŸÜŸá ÿßŸÑŸÖÿ≠ÿØÿØ ŸÅŸÇÿ∑ÿå ŸÑÿß ÿ™ŸÉÿ±ÿ± ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ŸÅŸä ÿ£ŸÖÿßŸÉŸÜ ÿ£ÿÆÿ±Ÿâ.\n"
        "8. ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿ© ŸÖÿ∑ŸÑŸàÿ®ÿ©ÿå ÿßŸÉÿ™ÿ® (ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±).\n"
    )

    return system, user


def split_text_into_chunks(text: str) -> List[str]:
    """Split text into chunks with overlap to maintain context."""
    if len(text) <= CHUNK_SIZE:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + CHUNK_SIZE

        # If not the last chunk, try to break at sentence boundary
        if end < len(text):
            # Look for sentence endings within last 200 chars
            search_start = max(start, end - 200)
            last_period = text.rfind(".", search_start, end)
            last_newline = text.rfind("\n", search_start, end)
            break_point = max(last_period, last_newline)

            if break_point > start:
                end = break_point + 1

        chunks.append(text[start:end])
        start = end - OVERLAP_SIZE if end < len(text) else end

    logger.info(f"Split text into {len(chunks)} chunks")
    return chunks


def call_bedrock_for_chunk(chunk_text: str, chunk_num: int, total_chunks: int) -> str:
    """Call Bedrock to rewrite a single chunk."""
    system_prompt, _ = build_rewrite_prompts("")

    user_prompt = (
        f"ÿ£ÿπÿØ ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÜ ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ (ÿßŸÑÿ¨ÿ≤ÿ° {chunk_num} ŸÖŸÜ {total_chunks}). "
        "ÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ≠ŸÇÿßÿ¶ŸÇ ŸàÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸàÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸÉŸÖÿß ŸáŸä.\n\n"
        f"{chunk_text}\n\n"
        "ÿßŸÉÿ™ÿ® ÿßŸÑŸÜÿ≥ÿÆÿ© ÿßŸÑŸÖÿπÿßÿØ ÿµŸäÿßÿ∫ÿ™Ÿáÿß ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ŸÅŸÇÿ∑."
    )

    request_body = {
        "system": [{"text": system_prompt}],
        "messages": [{"role": "user", "content": [{"text": user_prompt}]}],
        "inferenceConfig": {
            "maxTokens": MAX_TOKENS,
            "temperature": 0.0,
            "topP": 0.8
        }
    }

    try:
        response = bedrock_runtime.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(request_body, ensure_ascii=False).encode("utf-8")
        )

        response_body = json.loads(response["body"].read())
        stop_reason = response_body.get("stopReason", "")

        if isinstance(stop_reason, str) and ("content_filtered" in stop_reason.lower() or "blocked" in stop_reason.lower()):
            raise ValueError("Content blocked by safety filters")

        return response_body["output"]["message"]["content"][0]["text"]

    except Exception as e:
        logger.error(f"Chunk {chunk_num} failed: {e}")
        raise


def call_bedrock_for_rewrite(original_text: str) -> str:
    """Rewrite document by processing in chunks if needed."""
    chunks = split_text_into_chunks(original_text)

    if len(chunks) == 1:
        logger.info("Processing as single chunk")
        return call_bedrock_for_chunk(chunks[0], 1, 1)

    logger.info(f"Processing {len(chunks)} chunks")
    rewritten_chunks = []

    for i, chunk in enumerate(chunks, 1):
        logger.info(f"Processing chunk {i}/{len(chunks)}")
        rewritten = call_bedrock_for_chunk(chunk, i, len(chunks))
        rewritten_chunks.append(rewritten)

    # Merge chunks, removing overlap duplicates
    result = rewritten_chunks[0]
    for chunk in rewritten_chunks[1:]:
        # Simple merge - append with newline
        result += "\n" + chunk

    logger.info(f"Merged {len(chunks)} chunks. Final length: {len(result)}")
    return result


# ---------------------- Entity Extraction & Validation ----------------------

ROLE_KEYWORDS = [
    "ŸÖÿ®ŸÑÿ∫", "ŸÖÿØÿπŸâ ÿπŸÑŸäŸá", "ŸÖÿ¥ŸÉŸàŸãŸâ ÿπŸÑŸäŸá", "ŸÖÿ¥ŸÉŸà ÿπŸÑŸäŸá", "ÿ¥ÿßŸáÿØ", "ŸÖÿ¥ÿ™ÿ®Ÿá", "ŸÖÿ∞ŸÉŸàÿ±",
    "ŸÖÿ¨ŸÜŸä ÿπŸÑŸäŸá", "ŸÖÿ™ŸáŸÖ", "ŸÖÿ≠ÿßŸÖŸä", "ŸÇÿßÿ∂Ÿä", "ŸÖÿ≠ŸÇŸÇ", "ŸÉÿßÿ™ÿ® ÿ∂ÿ®ÿ∑"
]

SECTION_KEYWORDS = [
    "ŸÖŸÇÿØŸÖÿ©", "ŸÖŸÑÿÆÿµ", "ŸÖŸÑÿÆÿµ ÿßŸÑÿ≠ÿßÿØÿ´", "ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑŸàÿßŸÇÿπÿ©", "ŸàŸÇÿßÿ¶ÿπ ÿßŸÑŸÇÿ∂Ÿäÿ©", "ÿ£ŸÇŸàÿßŸÑ",
    "ÿßŸÑÿ¥ŸáŸàÿØ", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑÿ¥ÿßŸÉŸä", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿ®ŸÑÿ∫", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑÿ¥ÿßŸáÿØ",
    "ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ", "ÿßŸÑÿ™ŸÇÿ±Ÿäÿ±", "ÿßŸÑÿÆÿßÿ™ŸÖÿ©", "ÿßŸÑÿ∑ŸÑÿ®ÿßÿ™", "ÿßŸÑŸÇÿ±ÿßÿ±", "ÿ£ÿ∑ÿ±ÿßŸÅ ÿßŸÑÿ®ŸÑÿßÿ∫", "ŸÖÿ≥ÿ±ÿ≠ ÿßŸÑÿ≠ÿßÿØÿ´"
]


def extract_entities(text: str) -> Dict[str, Set[str]]:
    """Extract names, roles, case numbers, dates, times, IDs, locations from Arabic text."""
    names: Set[str] = set()

    # Naive Arabic name pattern (2-5 tokens of letters) ‚Äì conservative to reduce false positives
    for m in re.finditer(r"\b[\u0621-\u064A]{2,}(?:\s+[\u0621-\u064A]{2,}){1,4}\b", text):
        nm = m.group(0).strip()
        excluded_terms = (
            r"\b(ŸÖŸÖŸÑŸÉÿ©|Ÿàÿ≤ÿßÿ±ÿ©|ÿßŸÑŸÜŸäÿßÿ®ÿ©|ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜ|ÿ¥ÿ±ÿ∑ÿ©|ŸÇÿ±ÿßÿ±|ÿ®ŸÑÿßÿ∫|ÿßŸÑŸÇÿ∂Ÿäÿ©|ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ|ÿßŸÑŸÖÿ≠ŸÉŸÖÿ©|"
            r"ÿßŸÑÿ¨ŸÜÿßÿ¶Ÿäÿ©|ÿßŸÑÿπÿßŸÖÿ©|ÿßŸÑÿ£ŸÖŸÜ|ÿßŸÑÿπÿØŸÑ|ÿßŸÑŸÇÿßŸÜŸàŸÜ|ÿßŸÑÿ≠ŸÉŸàŸÖÿ©|ÿßŸÑÿØÿßÿÆŸÑŸäÿ©|ŸÜŸäÿßÿ®ÿ©|ŸÖÿ±ŸÉÿ≤ ÿ¥ÿ±ÿ∑ÿ©)\b"
        )
        if len(nm.split()) >= 2 and not re.search(excluded_terms, nm):
            names.add(nm)

    roles: Set[str] = set()
    for kw in ROLE_KEYWORDS:
        if re.search(rf"\b{re.escape(kw)}\b", text):
            roles.add(kw)

    # Case number patterns
    case_numbers: Set[str] = set(
        re.findall(r"(?:ÿ±ŸÇŸÖ\s*(?:ÿßŸÑÿ®ŸÑÿßÿ∫|ÿßŸÑŸÇÿ∂Ÿäÿ©)\s*[:Ôºö]?\s*(\d{2,}))", text)
    )

    # Dates: dd/mm/yyyy or dd-mm-yyyy or yyyy-mm-dd
    dates: Set[str] = set(
        re.findall(r"\b(?:\d{1,2}[\-/]\d{1,2}[\-/]\d{2,4}|\d{4}[\-/]\d{1,2}[\-/]\d{1,2})\b", text)
    )

    # Times: HH:MM(:SS)
    times: Set[str] = set(re.findall(r"\b\d{1,2}:\d{2}(?::\d{2})?\b", text))

    # National IDs: 9-12 digits
    national_ids: Set[str] = set(re.findall(r"\b\d{9,12}\b", text))

    # Locations: very crude detection via ÿ®ÿπÿØ 'ŸÅŸä'/'ÿ®ŸÄ'
    locations: Set[str] = set()
    for m in re.finditer(
        r"\b(?:ŸÅŸä|ÿ®ŸÄ)\s+([\u0621-\u064A]{2,}(?:\s+[\u0621-\u064A]{2,}){0,3})\b", text
    ):
        loc = m.group(1).strip()
        if not re.search(r"\b(ÿßŸÑŸÖÿ∞ŸÉŸàÿ±|ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ©|ÿßŸÑŸÖÿØÿπŸâ|ÿßŸÑÿ¥ÿßŸÉŸä|ÿßŸÑŸÖÿ™ŸáŸÖ)\b", loc):
            locations.add(loc)

    sections: Set[str] = set()
    for kw in SECTION_KEYWORDS:
        # Allow heading followed by colon and either space or newline
        pattern = rf"\n\s*{re.escape(kw)}\s*[:Ôºö]?(?:\s|\n)"
        if re.search(pattern, text):
            sections.add(kw)

    return {
        "names": names,
        "roles": roles,
        "case_numbers": case_numbers,
        "dates": dates,
        "times": times,
        "national_ids": national_ids,
        "locations": locations,
        "sections": sections
    }


def remove_duplicated_paragraphs(text: str) -> str:
    """Remove exact duplicate paragraphs that often appear due to artifacts."""
    paras = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]
    seen: Set[str] = set()
    result: List[str] = []

    for p in paras:
        key = hashlib.md5(p.encode("utf-8")).hexdigest()
        if key not in seen:
            seen.add(key)
            result.append(p)

    return "\n\n".join(result)


def case_boundary_protection(text: str) -> str:
    """
    Light cleanup to remove obviously fabricated headings but do NOT truncate
    valid multi-page reports or repeated headers like ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫.
    """
    forbidden_heads = [
        r"^\s*ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑÿ±ÿ≥ŸÖŸä\s*$",
        r"^\s*ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ\s*[:Ôºö]?\s*$"
    ]

    lines = text.splitlines()
    cleaned_lines: List[str] = []

    for ln in lines:
        if any(re.match(pat, ln.strip()) for pat in forbidden_heads):
            continue
        cleaned_lines.append(ln)

    return "\n".join(cleaned_lines)


def validate_and_sanitize(original: str, rewritten: str) -> Tuple[bool, str, List[str]]:
    """
    Validate rewritten text against original entities and structure.
    Returns (is_valid, sanitized_text, violations).
    If invalid, sanitized_text is cleaned.
    """
    orig = extract_entities(original)
    new = extract_entities(rewritten)
    violations: List[str] = []

    # New names introduced
    extra_names = new["names"] - orig["names"]
    if extra_names:
        violations.append(f"ÿ£ÿ≥ŸÖÿßÿ° ÿ¨ÿØŸäÿØÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ£ÿµŸÑ: {', '.join(sorted(extra_names))}")

    # New roles introduced
    extra_roles = new["roles"] - orig["roles"]
    if extra_roles:
        violations.append(f"ÿ£ÿØŸàÿßÿ± ÿ¨ÿØŸäÿØÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ£ÿµŸÑ: {', '.join(sorted(extra_roles))}")

    # Case number duplication/new
    if len(new["case_numbers"]) > len(orig["case_numbers"]) or (
        new["case_numbers"] - orig["case_numbers"]
    ):
        violations.append("ÿ±ŸÇŸÖ ÿ®ŸÑÿßÿ∫/ŸÇÿ∂Ÿäÿ© ÿ•ÿ∂ÿßŸÅŸä ÿ£Ÿà ŸÖÿÆÿ™ŸÑŸÅ ÿ™ŸÖ ÿ•ÿØÿÆÿßŸÑŸá")

    # Missing critical info: names or case numbers removed
    missing_names = orig["names"] - new["names"]
    if missing_names:
        violations.append(
            f"ÿ™ŸÖ ÿ≠ÿ∞ŸÅ ÿ®ÿπÿ∂ ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸÖŸÜ ÿßŸÑŸÜÿµ: {', '.join(sorted(missing_names))}"
        )

    missing_case_numbers = orig["case_numbers"] - new["case_numbers"]
    if missing_case_numbers:
        violations.append("ÿ™ŸÖ ÿ≠ÿ∞ŸÅ ÿ±ŸÇŸÖ ÿ®ŸÑÿßÿ∫/ŸÇÿ∂Ÿäÿ© ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä")

    # Sections fabricated
    fabricated_sections = new["sections"] - orig["sections"]
    if fabricated_sections:
        violations.append(
            f"ÿ£ŸÇÿ≥ÿßŸÖ ŸÖŸèÿ∂ÿßŸÅÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ£ÿµŸÑ: {', '.join(sorted(fabricated_sections))}"
        )

    # Detect duplicated paragraphs
    para_counts: Dict[str, int] = {}
    for p in re.split(r"\n{2,}", rewritten):
        p = p.strip()
        if not p:
            continue
        para_counts[p] = para_counts.get(p, 0) + 1

    if any(c > 1 for c in para_counts.values()):
        violations.append("ÿ™ŸÉÿ±ÿßÿ± ŸÅŸÇÿ±ÿßÿ™")

    # Sanitization steps: remove duplicates, run light boundary protection
    sanitized = remove_duplicated_paragraphs(rewritten)
    sanitized = case_boundary_protection(sanitized)

    # If violations exist, try removing some obviously fabricated lines
    if violations:
        lines = sanitized.splitlines()
        kept: List[str] = []
        for ln in lines:
            if re.search(r"\b(?:ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑÿ±ÿ≥ŸÖŸä|ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ)\b", ln):
                continue
            kept.append(ln)
        sanitized = "\n".join(kept)

    is_valid = len(violations) == 0
    return is_valid, sanitized, violations


def update_job_status(job_id: str, status: str, data: Optional[Dict] = None) -> None:
    """Update job status in S3."""
    status_key = f"rewrite-jobs/{job_id}/status.json"
    
    status_data = {
        "jobId": job_id,
        "status": status,
        "updatedAt": datetime.utcnow().isoformat()
    }
    
    if data:
        status_data.update(data)
    
    s3_client.put_object(
        Bucket=BUCKET_NAME,
        Key=status_key,
        Body=json.dumps(status_data, ensure_ascii=False),
        ContentType="application/json"
    )
    
    logger.info(f"Updated job {job_id} status to {status}")


def save_rewritten_result(job_id: str, rewritten_text: str, original_length: int) -> str:
    """Save the rewritten text to S3 and return the key."""
    result_key = f"rewrite-jobs/{job_id}/result.txt"
    
    s3_client.put_object(
        Bucket=BUCKET_NAME,
        Key=result_key,
        Body=rewritten_text.encode("utf-8"),
        ContentType="text/plain; charset=utf-8",
        Metadata={
            "jobId": job_id,
            "originalLength": str(original_length),
            "rewrittenLength": str(len(rewritten_text))
        }
    )
    
    logger.info(f"Saved rewritten result for job {job_id} to {result_key}")
    return result_key


def lambda_handler(event: Dict, context: Any) -> None:
    """
    Worker Lambda handler - processes rewrite jobs asynchronously.
    Does not return a response to API Gateway.
    """
    job_id = None
    
    try:
        # Extract job details from event (sent by Lambda 1)
        job_id = event.get("jobId")
        text = event.get("text")
        s3_key = event.get("s3Key")
        session_id = event.get("sessionId", "unknown")
        
        if not job_id:
            logger.error("No job ID provided in event")
            return
        
        logger.info(f"üîÑ Processing rewrite job {job_id} for session {session_id}")
        
        # Get input text
        if not text:
            if s3_key:
                text = read_text_from_s3(BUCKET_NAME, s3_key)
                logger.info(f"‚û° Loaded text from S3: {get_safe_log_info(text, session_id)}")
            else:
                logger.error(f"No text or s3Key provided for job {job_id}")
                update_job_status(job_id, "FAILED", {
                    "error": "No text or s3Key provided",
                    "sessionId": session_id
                })
                return
        
        # Validate text size
        if len(text) > MAX_TOTAL_CHARS:
            logger.warning(f"Job {job_id}: Text too long ({len(text)} chars)")
            update_job_status(job_id, "FAILED", {
                "error": f"Text too long. Max {MAX_TOTAL_CHARS} chars allowed",
                "currentChars": len(text),
                "sessionId": session_id
            })
            return
        
        # Perform rewrite
        logger.info(f"Starting Bedrock processing for job {job_id}")
        bedrock_output = call_bedrock_for_rewrite(text)
        
        # Validate and sanitize
        is_valid, sanitized, violations = validate_and_sanitize(text, bedrock_output)
        
        if not is_valid:
            logger.warning(f"Job {job_id}: Validation violations: {violations}")
            rewritten_text = sanitized
        else:
            rewritten_text = bedrock_output
        
        # Save result to S3
        result_key = save_rewritten_result(job_id, rewritten_text, len(text))
        
        # Update status to COMPLETED
        update_job_status(job_id, "COMPLETED", {
            "resultKey": result_key,
            "resultLength": len(rewritten_text),
            "originalLength": len(text),
            "model": MODEL_ID,
            "sessionId": session_id,
            "validationPassed": is_valid,
            "violations": violations if not is_valid else []
        })
        
        logger.info(f"‚úÖ Job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Error processing job {job_id}: {e}", exc_info=True)
        
        if job_id:
            update_job_status(job_id, "FAILED", {
                "error": str(e),
                "errorType": type(e).__name__
            })
