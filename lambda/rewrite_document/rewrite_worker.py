"""
Lambda 2: Rewrite Worker
Performs the actual rewrite operation asynchronously and stores the result in S3.
This is invoked by Lambda 1 and does not return a response to API Gateway.
"""

import json
import boto3
import os
import logging
import hashlib
import re
from datetime import datetime
from typing import Dict, Tuple, Any, Optional, List, Set

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS clients
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-east-1")
s3_client = boto3.client("s3")

# Configuration
BUCKET_NAME = os.environ.get("BUCKET_NAME", "vision-investigation-system-052904446370")
MODEL_ID = "amazon.nova-lite-v1:0"

# ===== Size & performance limits =====
MAX_TOTAL_CHARS = 60000  # Max chars for entire document
CHUNK_SIZE = 15000  # Characters per chunk
OVERLAP_SIZE = 500  # Overlap between chunks for context
MAX_TOKENS = 4000  # Bedrock output limit per chunk


def get_safe_log_info(text: str, session_id: Optional[str] = None) -> Dict[str, Any]:
    """Return safe hashed log info without exposing private data."""
    return {
        "text_hash": hashlib.md5(text.encode("utf-8")).hexdigest()[:8],
        "text_length": len(text),
        "session_id": session_id or "unknown"
    }


def find_latest_extracted_folder(bucket: str) -> Optional[str]:
    """Find the most recently modified folder in classification/extracted/."""
    try:
        prefix = "classification/extracted/"
        logger.info(f"üîç Finding latest folder in: s3://{bucket}/{prefix}")
        
        response = s3_client.list_objects_v2(
            Bucket=bucket,
            Prefix=prefix,
            Delimiter='/'
        )
        
        if 'CommonPrefixes' not in response:
            logger.warning(f"‚ùå No folders found in s3://{bucket}/{prefix}")
            return None
        
        folders = [p['Prefix'] for p in response['CommonPrefixes']]
        logger.info(f"üìÇ Found {len(folders)} folders")
        
        if not folders:
            return None
        
        # Get the last modified time of each folder by checking its contents
        folder_times = []
        for folder in folders:
            folder_response = s3_client.list_objects_v2(
                Bucket=bucket,
                Prefix=folder,
                MaxKeys=1
            )
            if 'Contents' in folder_response and folder_response['Contents']:
                last_modified = folder_response['Contents'][0]['LastModified']
                folder_times.append((folder, last_modified))
                logger.info(f"  üìÅ {folder} - Last modified: {last_modified}")
        
        if not folder_times:
            logger.warning("No folders with contents found")
            return None
        
        # Sort by last modified time, most recent first
        folder_times.sort(key=lambda x: x[1], reverse=True)
        latest_folder = folder_times[0][0]
        
        logger.info(f"‚úÖ Latest folder: {latest_folder}")
        return latest_folder
        
    except Exception as e:
        logger.error(f"‚ùå Failed to find latest folder: {e}", exc_info=True)
        return None


def find_text_file_in_folder(bucket: str, folder_prefix: str) -> Optional[str]:
    """Find the first .txt file in an S3 folder."""
    try:
        # Ensure folder prefix ends with /
        if not folder_prefix.endswith('/'):
            folder_prefix += '/'
        
        logger.info(f"üîç Searching for .txt files in: s3://{bucket}/{folder_prefix}")
        
        response = s3_client.list_objects_v2(
            Bucket=bucket,
            Prefix=folder_prefix,
            MaxKeys=100
        )
        
        if 'Contents' not in response:
            logger.warning(f"‚ùå No files found in s3://{bucket}/{folder_prefix}")
            logger.warning(f"üìù Response: {response}")
            return None
        
        # Log all found objects for debugging
        logger.info(f"üìÇ Found {len(response['Contents'])} objects:")
        for obj in response['Contents']:
            logger.info(f"  - {obj['Key']}")
        
        # Find first .txt file
        for obj in response['Contents']:
            key = obj['Key']
            if key.endswith('.txt') and not key.endswith('/'):
                logger.info(f"‚úÖ Found text file: {key}")
                return key
        
        logger.warning(f"‚ö†Ô∏è No .txt files found in s3://{bucket}/{folder_prefix}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Failed to list files in s3://{bucket}/{folder_prefix} - {e}")
        logger.error(f"Error details: {str(e)}", exc_info=True)
        return None


def read_text_from_s3(bucket: str, key: str) -> str:
    """Read text file from S3 bucket."""
    try:
        obj = s3_client.get_object(Bucket=bucket, Key=key)
        return obj["Body"].read().decode("utf-8")
    except Exception as e:
        logger.error(f"‚ùå Failed to read s3://{bucket}/{key} - {e}")
        raise


def build_rewrite_prompts(original_text: str) -> Tuple[str, str]:
    system = (
        "ÿ£ŸÜÿ™ ŸÖÿ≠ÿ±ŸëŸêÿ± ÿ™ŸÇÿßÿ±Ÿäÿ± ÿ¨ŸÜÿßÿ¶Ÿäÿ© ŸäÿπŸÖŸÑ ŸÑÿµÿßŸÑÿ≠ ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ© ŸÅŸä ŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜ.\n"
        "ŸÖÿµÿØÿ± ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸáŸà ŸÖŸÑŸÅ ÿ®ŸÑÿßÿ∫ ÿ¥ÿ±ÿ∑Ÿä/ŸÇÿ∂Ÿäÿ© ŸÜŸäÿßÿ®ÿ© Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ¨ÿØÿßŸàŸÑ ŸÖÿ∑ÿ®Ÿàÿπÿ© "
        "ŸàŸÖÿ¨ŸÖŸàÿπÿ© ŸÖÿ≠ÿßÿ∂ÿ± ÿ®ÿπŸÜŸàÿßŸÜ (ŸÅÿ™ÿ≠ ÿßŸÑŸÖÿ≠ÿ∂ÿ± ÿ®ÿßŸÑÿ™ÿßÿ±ŸäÿÆ ŸàÿßŸÑŸàŸÇÿ™ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ŸäŸÜ ÿ£ÿπŸÑÿßŸá...).\n\n"
        "ŸÖŸáŸÖÿ™ŸÉ:\n"
        "- ÿ•ŸÜÿ™ÿßÿ¨ ÿ™ŸÇÿ±Ÿäÿ± Ÿàÿßÿ≠ÿØ ŸÖŸÜÿ∏ŸÖ ŸàŸàÿßÿ∂ÿ≠ ŸÅŸÇÿ∑ÿå ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ±ÿ≥ŸÖŸäÿ©ÿå ÿ®ÿØŸàŸÜ ÿ£Ÿä ÿ≤ÿÆÿ±ŸÅÿ©.\n"
        "- ŸÑÿß ÿ™ÿ∂ŸÅ ÿ£Ÿä ŸÖÿπŸÑŸàŸÖÿ© ÿ¨ÿØŸäÿØÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä.\n"
        "- ŸÑÿß ÿ™ÿ≠ÿ∞ŸÅ ÿ£Ÿä ŸÖÿπŸÑŸàŸÖÿ© ÿ¨ŸàŸáÿ±Ÿäÿ© ŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿßŸÑŸÇÿ∂Ÿäÿ© (ÿ£ÿ≥ŸÖÿßÿ°ÿå ÿ£ÿ±ŸÇÿßŸÖ ÿ¥ÿÆÿµŸäÿ©ÿå ÿ™Ÿàÿßÿ±ŸäÿÆÿå ÿ£ŸàŸÇÿßÿ™ÿå ÿ£ŸÖÿßŸÉŸÜÿå ÿ£ŸÇŸàÿßŸÑÿå ŸÇÿ±ÿßÿ±ÿßÿ™ÿå ÿ£ÿ±ŸÇÿßŸÖ ÿ®ŸÑÿßÿ∫ÿßÿ™ÿå ÿ≠ÿßŸÑÿ© ÿßŸÑŸÖÿ™ŸáŸÖÿå ÿ≠ÿßŸÑÿ© ÿßŸÑÿµŸÑÿ≠ÿå Ÿàÿ¨ŸàÿØ ÿ™ÿµŸàŸäÿ±ÿå ÿ•ŸÑÿÆ).\n"
        "- Ÿäÿ¨Ÿàÿ≤ ŸÑŸÉ ÿ≠ÿ∞ŸÅ ÿßŸÑÿ≥ÿ∑Ÿàÿ± ÿßŸÑŸÖŸÉÿ±ÿ±ÿ© (ŸÖÿ´ŸÑ ÿ™ŸÉÿ±ÿßÿ± ÿ±ÿ£ÿ≥ ÿßŸÑÿµŸÅÿ≠ÿ©ÿå ÿ¨ŸÖŸÑÿ© \"ŸÇÿ∂Ÿäÿ© ŸÜŸäÿßÿ®ÿ© / ÿ¨ŸÜÿßÿ¶Ÿä / ÿ¨ŸÜÿßÿ¶Ÿä ÿπÿßŸÖ ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫\"ÿå ÿ£Ÿà ÿ™ŸÉÿ±ÿßÿ± ŸÜŸÅÿ≥ ÿßŸÑŸÅŸÇÿ±ÿ© ŸÜÿµÿßŸã).\n"
        "- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÜŸÅÿ≥ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ŸÖŸÉÿ±ÿ±ÿ© ŸÅŸä ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖŸÉÿßŸÜ (ŸÖÿ´ŸÑÿßŸã ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫ ÿ£Ÿà ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ)ÿå ÿßÿ∞ŸÉÿ±Ÿáÿß ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸä ÿßŸÑŸÇÿ≥ŸÖ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®.\n"
        "- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ÿ¨ŸÖŸÑ ŸÖŸÇÿ∑Ÿàÿπÿ© ÿ£Ÿà ÿ∫Ÿäÿ± ŸÖŸÅŸáŸàŸÖÿ© ÿ®ÿ≥ÿ®ÿ® OCR ŸàŸÑÿß ŸäŸÖŸÉŸÜ ŸÅŸáŸÖ ŸÖÿπŸÜÿßŸáÿßÿå Ÿäÿ¨Ÿàÿ≤ ÿ≠ÿ∞ŸÅŸáÿß ÿØŸàŸÜ ÿ™ÿÆŸÖŸäŸÜ.\n"
        "- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖŸÑŸÅ ÿ£Ÿà ÿ•ÿ¨ÿ±ÿßÿ° ÿ∫Ÿäÿ± ŸÖÿ±ÿ™ÿ®ÿ∑ ÿ®ÿßŸÑŸÇÿ∂Ÿäÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©ÿå ÿ∂ÿπ Ÿáÿ∞Ÿá ÿßŸÑÿ£ÿ¨ÿ≤ÿßÿ° ŸÅŸä ŸÇÿ≥ŸÖ (ŸÖŸÑÿßÿ≠ŸÇ ÿ•ÿ∂ÿßŸÅŸäÿ©) ŸÅŸÇÿ∑ ŸàŸÑÿß ÿ™ÿØŸÖÿ¨Ÿáÿß ŸÅŸä ÿµŸÑÿ® ÿßŸÑÿ™ŸÇÿ±Ÿäÿ±.\n"
        "- ŸÑÿß ÿ™ÿπŸäÿØ ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖÿ±ÿ©.\n"
        "- ŸÑÿß ÿ™ÿπŸäÿØ ÿµŸäÿßÿ∫ÿ© ŸÜŸÅÿ≥ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿ®ÿµŸäÿ∫ÿ™ŸäŸÜ ŸÖÿÆÿ™ŸÑŸÅÿ™ŸäŸÜ.\n\n"
        "ŸÇŸàÿßÿπÿØ ÿßŸÑÿ¨ÿØÿßŸàŸÑ ŸàÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:\n"
        "- ŸÖÿ≥ŸÖŸàÿ≠ ŸÅŸÇÿ∑ ÿ®ÿ¨ÿØŸàŸÑ Markdown Ÿàÿßÿ≠ÿØ ŸÑŸÇÿ≥ŸÖ (ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ) Ÿäÿ™ÿ∂ŸÖŸÜ ÿßŸÑÿµŸÅÿ© ŸàÿßŸÑÿßÿ≥ŸÖ ŸàÿßŸÑÿ±ŸÇŸÖ ÿßŸÑÿ¥ÿÆÿµŸä Ÿàÿ®ÿßŸÇŸä ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™.\n"
        "- ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑÿ£ÿÆÿ±Ÿâ (ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ±ÿå ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆÿå ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©ÿå ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿå ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™) ÿ™ŸèŸÉÿ™ÿ® ŸÉŸÜÿµ ÿ£Ÿà ŸÜŸÇÿßÿ∑ÿå ŸàŸÑŸäÿ≥ÿ™ ÿ¨ÿØÿßŸàŸÑ.\n"
        "- ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ÿ™ÿµŸÖŸäŸÖ ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿßŸÑÿ£ÿµŸÑŸäÿ© ŸÉŸÖÿß ŸáŸàÿõ ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸàÿßŸÖÿ≤ÿ¨Ÿáÿß ŸÅŸä ÿßŸÑŸÜÿµ ÿ£Ÿà ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ©.\n"
        "- ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ÿ¨ÿØÿßŸàŸÑ ÿ•ÿØÿÆÿßŸÑ ÿßŸÑŸÜÿ∏ÿßŸÖ ŸÖÿ´ŸÑ: (ÿ£ÿ∑ÿ±ÿßŸÅ ÿßŸÑÿ®ŸÑÿßÿ∫) ÿ£Ÿà (ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑÿπŸäŸÜŸäÿ©) ÿ£Ÿà (ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©) ÿ®ŸÜŸÅÿ≥ ÿ¥ŸÉŸÑŸáÿß.\n"
        "- ÿßŸÑÿ£ŸÇŸàÿßŸÑ (ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿ®ŸÑÿ∫ÿå ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸáÿå ÿßŸÑÿ¥ŸáŸàÿØ) ÿ™ŸèŸÉÿ™ÿ® ŸÅŸä ŸÅŸÇÿ±ÿßÿ™ ŸÜÿµŸäÿ©ÿå ŸÑŸäÿ≥ÿ™ ŸÅŸä ÿ¨ÿØŸàŸÑ.\n"
        "- ŸÖÿ≠ÿßÿ∂ÿ± ÿßŸÑÿ¥ÿ±ÿ∑ÿ© ŸÉŸÑŸáÿß (ŸÅÿ™ÿ≠ ÿßŸÑŸÖÿ≠ÿ∂ÿ±ÿå ÿßŸÜÿ™ŸÇÿßŸÑ ÿßŸÑŸÖŸàŸÇÿπÿå ÿßÿ≥ÿ™ÿπŸÑÿßŸÖÿßÿ™ÿå ÿ®ÿ≠ÿ´ Ÿàÿ™ÿ≠ÿ±Ÿä...) ÿ™ŸèŸÉÿ™ÿ® ŸÉŸÜÿµ ÿ£Ÿà ŸÜŸÇÿßÿ∑ ŸÅŸÇÿ∑ ÿ®ŸÑÿß ÿ£Ÿä ÿ¨ÿØŸàŸÑ.\n"
        "- ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ•ÿØÿßÿ±Ÿäÿ© ÿßŸÑÿπÿßŸÖÿ© (ŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜÿå ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ©ÿå ŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßÿµŸÖÿ©...) ÿ™Ÿèÿ∞ŸÉÿ± ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸä ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÇÿ∂Ÿäÿ© ŸÅŸÇÿ∑.\n"
        "- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿπŸÜÿßŸàŸäŸÜ ÿßŸÑŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑÿ´ÿßŸÜŸä Markdown ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ¥ŸÉŸÑ ŸÅŸÇÿ∑: '## ÿßŸÑÿπŸÜŸàÿßŸÜ'.\n"
        "- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿπŸÜÿßŸàŸäŸÜ ÿ®ŸÖÿ≥ÿ™ŸàŸäÿßÿ™ ÿ£ÿÆÿ±Ÿâ ŸÖÿ´ŸÑ '###' ÿ£Ÿà '####'.\n"
        "- ŸÑÿß ÿ™ŸÉÿ™ÿ® ÿπŸÜÿßŸàŸäŸÜ ÿ™ÿ®ÿØÿ£ ÿ®ŸÜŸÖÿ∑ ÿ∫ÿ±Ÿäÿ® ŸÖÿ´ŸÑ '#### ÿßŸÑ-'.\n\n"
        "ÿ∂Ÿàÿßÿ®ÿ∑ ÿ≠ÿ≥ÿßÿ≥ÿ©:\n"
        "- ŸÖŸÖŸÜŸàÿπ ÿßÿÆÿ™ÿ±ÿßÿπ ÿ£ÿ≥ŸÖÿßÿ° ÿ£ÿ¥ÿÆÿßÿµ ÿ£Ÿà ÿ¨Ÿáÿßÿ™ ÿ£Ÿà ÿ£ÿ±ŸÇÿßŸÖ ÿ¥ÿÆÿµŸäÿ© ÿ£Ÿà ÿ£ÿ±ŸÇÿßŸÖ ÿ®ŸÑÿßÿ∫ÿßÿ™ ÿ£Ÿà ŸÖÿ®ÿßŸÑÿ∫ ŸÖÿßŸÑŸäÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ©.\n"
        "- ŸÖŸÖŸÜŸàÿπ ÿ™ÿ∫ŸäŸäÿ± ÿ≠ÿßŸÑÿ© ÿßŸÑŸÖÿ™ŸáŸÖ (ŸÖŸàŸÇŸàŸÅ/ŸÖÿ∑ŸÑŸàÿ®/ŸÖÿÆŸÑŸâ ÿ≥ÿ®ŸäŸÑ) ÿ•ŸÑÿß ŸÉŸÖÿß Ÿàÿ±ÿØ ŸÅŸä ÿßŸÑŸÜÿµ.\n"
        "- ŸÖŸÖŸÜŸàÿπ ÿßÿÆÿ™ÿ±ÿßÿπ ŸÇÿ±ÿßÿ± ŸÜŸäÿßÿ®ÿ© ÿ£Ÿà ÿ≠ŸÉŸÖ ŸÖÿ≠ŸÉŸÖÿ© ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿ© ŸÖÿ∑ŸÑŸàÿ®ÿ© ŸÅŸä ÿßŸÑŸáŸäŸÉŸÑÿå ÿßŸÉÿ™ÿ® ÿ®ÿØŸÑÿßŸã ŸÖŸÜŸáÿß: 'ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ'.\n"
        "- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿ®ŸÑÿßÿ∫ ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿ±ŸÇŸÖ ŸÇÿ∂Ÿäÿ© ŸÖÿ∞ŸÉŸàÿ±ÿå ÿØŸàŸëŸêŸÜŸáÿß ŸÉŸÑŸáÿß ŸÅŸä ŸÇÿ≥ŸÖ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÇÿ∂Ÿäÿ© ÿ£Ÿà ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸáŸÖÿ© ŸÖÿπ ÿ™Ÿàÿ∂Ÿäÿ≠ ÿπŸÑÿßŸÇÿ™Ÿáÿß ŸÇÿØÿ± ÿßŸÑÿ•ŸÖŸÉÿßŸÜ ŸÖŸÜ ÿßŸÑŸÜÿµ ŸÜŸÅÿ≥Ÿá ŸÅŸÇÿ∑.\n"
    )

    user = (
        "ÿ•ŸÑŸäŸÉ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä ÿßŸÑŸÉÿßŸÖŸÑ ŸÑŸÖŸÑŸÅ ÿ®ŸÑÿßÿ∫/ŸÇÿ∂Ÿäÿ© ŸÇÿßÿØŸÖ ŸÖŸÜ ŸÖÿ±ŸÉÿ≤ ÿ¥ÿ±ÿ∑ÿ©/ŸÜŸäÿßÿ®ÿ©:\n\n"
        f"{original_text}\n\n"
        "ÿßŸÑŸÖÿ∑ŸÑŸàÿ®: ÿ•ÿπÿßÿØÿ© ÿ™ŸÜÿ∏ŸäŸÖ ŸàŸÉÿ™ÿßÿ®ÿ© ÿ™ŸÇÿ±Ÿäÿ± Ÿàÿßÿ≠ÿØ ŸÅŸÇÿ∑ ÿ®ÿµŸäÿ∫ÿ© Ÿàÿßÿ∂ÿ≠ÿ© ŸàŸÖŸÜÿ≥ŸÇÿ©ÿå "
        "ŸàŸÅŸÇ ÿßŸÑŸáŸäŸÉŸÑ ÿßŸÑÿ™ÿßŸÑŸä ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ÿå ŸÖÿπ ŸÖŸÑÿ° ÿ£ŸÉÿ®ÿ± ŸÇÿØÿ± ŸÖŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑÿ≠ŸÇŸàŸÑ ŸÖŸÜ ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä "
        "ÿØŸàŸÜ ÿßÿÆÿ™ÿ±ÿßÿπ ÿ£Ÿä ŸÖÿπŸÑŸàŸÖÿ©.\n\n"
        "ÿßÿ≥ÿ™ÿÆÿØŸÖ Ÿáÿ∞ÿß ÿßŸÑŸáŸäŸÉŸÑ ŸÉŸÖÿß ŸáŸà (Ÿàÿ®ŸÜŸÅÿ≥ ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑÿπŸÜÿßŸàŸäŸÜ):\n\n"
        "## ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÇÿ∂Ÿäÿ©\n\n"
        "- ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä / ÿ±ŸÇŸÖ ÿßŸÑŸÇÿ∂Ÿäÿ© (ŸÖÿ´ÿßŸÑ: 13227/2025):\n"
        "- ŸÜŸàÿπ ÿßŸÑŸÇÿ∂Ÿäÿ© / ÿ™ÿµŸÜŸäŸÅŸáÿß (ŸÖÿ´ÿßŸÑ: ÿ≥ÿ±ŸÇÿ©ÿå ÿ¨ŸÜÿßÿ¶Ÿä ÿπÿßŸÖÿå ÿ¨ŸÜÿ≠ÿ© ...):\n"
        "- ŸÖÿ±ŸÉÿ≤ ÿßŸÑÿ¥ÿ±ÿ∑ÿ© ÿßŸÑŸÖÿ≠ŸäŸÑ (ŸÖÿ´ÿßŸÑ: ŸÖÿ±ŸÉÿ≤ ÿ¥ÿ±ÿ∑ÿ© ÿ¨ŸÜŸàÿ® ÿßŸÑÿπÿßÿµŸÖÿ©):\n"
        "- ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑŸÖÿÆÿ™ÿµÿ© (ŸÖÿ´ÿßŸÑ: ŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßÿµŸÖÿ©):\n"
        "- ÿ™ÿßÿ±ŸäÿÆ ŸàŸàŸÇÿ™ ÿ™ŸÑŸÇŸä ÿßŸÑÿ®ŸÑÿßÿ∫ ÿßŸÑÿ£ŸàŸÑ:\n"
        "- ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫ÿßÿ™ ÿ£Ÿà ÿßŸÑÿ™ÿπŸÖŸäŸÖÿßÿ™ ÿßŸÑÿ£ÿÆÿ±Ÿâ ÿßŸÑŸÖÿ±ÿ™ÿ®ÿ∑ÿ© (ÿ•ŸÜ Ÿàÿ¨ÿØÿ™):\n"
        "- Ÿàÿ¨ŸàÿØ ÿ™ÿµŸàŸäÿ± ÿ£ŸÖŸÜŸä (ŸÜÿπŸÖ / ŸÑÿß / ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±):\n"
        "- ÿ±ÿ∫ÿ®ÿ© ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ ŸÅŸä ÿßŸÑÿµŸÑÿ≠ (ŸÜÿπŸÖ / ŸÑÿß / ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±):\n\n"
        "## ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ\n\n"
        "ÿßŸÉÿ™ÿ® ÿ¨ÿØŸàŸÑ Markdown Ÿàÿßÿ≠ÿØ ŸÑŸÉŸÑ ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ŸäŸÜ ŸÅŸä ÿßŸÑŸÖŸÑŸÅÿå "
        "ÿ®ÿ∫ÿ∂ ÿßŸÑŸÜÿ∏ÿ± ÿπŸÜ ŸÖŸÉÿßŸÜ ÿ∏ŸáŸàÿ±ŸáŸÖ ŸÅŸä ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿ£Ÿà ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±. Ÿáÿ∞ÿß ŸáŸà **ÿßŸÑÿ¨ÿØŸàŸÑ ÿßŸÑŸàÿ≠ŸäÿØ ÿßŸÑŸÖÿ≥ŸÖŸàÿ≠** ŸÅŸä ÿßŸÑÿ™ŸÇÿ±Ÿäÿ±:\n\n"
        "| ÿßŸÑÿµŸÅÿ© | ÿßŸÑÿßÿ≥ŸÖ ÿßŸÑŸÉÿßŸÖŸÑ | ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ© | ÿßŸÑÿ±ŸÇŸÖ ÿßŸÑÿ¥ÿÆÿµŸä | ÿßŸÑŸáÿßÿ™ŸÅ | ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ |\n"
        "| --- | --- | --- | --- | --- | --- |\n"
        "| (ÿßŸÑŸÖÿ®ŸÑÿ∫ / ÿßŸÑŸÖÿ™ÿ∂ÿ±ÿ± / ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá / ÿ¥ÿßŸáÿØ / ÿ∂ÿßÿ®ÿ∑ / ÿ±ÿ¶Ÿäÿ≥ ÿ¨ŸÖÿπŸäÿ© / ÿ•ŸÑÿÆ) | (ÿßŸÑÿßÿ≥ŸÖ ŸÉŸÖÿß Ÿàÿ±ÿØ) | (ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©) | (ÿßŸÑÿ±ŸÇŸÖ ÿßŸÑÿ¥ÿÆÿµŸä ÿ•ŸÜ Ÿàÿ¨ÿØ) | (ÿ±ŸÇŸÖ ÿßŸÑŸáÿßÿ™ŸÅ ÿ•ŸÜ Ÿàÿ¨ÿØ) | (ŸÖÿ´ŸÑ: ŸÖŸàŸÇŸàŸÅÿå ŸÖÿ∑ŸÑŸàÿ®ÿå ÿ¨ŸÖÿπŸäÿ©ÿå ÿ¨Ÿáÿ© ÿπŸÖŸÑÿå ÿ•ŸÑÿÆ) |\n\n"
        "ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸÇŸàŸÑ ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµÿå ÿßŸÉÿ™ÿ®: 'ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±'.\n\n"
        "## ŸÖŸÑÿÆÿµ ÿßŸÑÿ≠ÿßÿØÿ´\n\n"
        "- ŸÑÿÆÿµ ÿßŸÑÿ≠ÿßÿØÿ´ ŸÅŸä ŸÅŸÇÿ±ÿ© ÿ£Ÿà ŸÅŸÇÿ±ÿ™ŸäŸÜ: ÿßŸÑÿ≤ŸÖÿßŸÜÿå ÿßŸÑŸÖŸÉÿßŸÜÿå ŸÖÿß ÿßŸÑÿ∞Ÿä ÿ≠ÿØÿ´ÿå "
        "ŸàŸÖÿß ŸáŸä ÿßŸÑÿ™ŸáŸÖÿ© ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© ÿßŸÑŸÖŸÜÿ≥Ÿàÿ®ÿ© ÿ•ŸÑŸâ ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá.\n\n"
        "## ŸÖÿ≥ÿ±ÿ≠ ÿßŸÑÿ≠ÿßÿØÿ´\n\n"
        "- ÿßÿ∞ŸÉÿ± ÿπŸÜŸàÿßŸÜ ŸÖŸàŸÇÿπ ÿßŸÑÿ≠ÿßÿØÿ´ ŸÉŸÖÿß Ÿàÿ±ÿØ (ŸÖŸÜÿ∑ŸÇÿ©ÿå ŸÖÿ¨ŸÖÿπÿå ÿ¥ÿßÿ±ÿπÿå ŸÖÿ®ŸÜŸâÿå ŸàÿµŸÅ ÿ•ÿ∂ÿßŸÅŸä).\n"
        "- ÿ•ÿ∞ÿß ÿ∞ŸèŸÉÿ± ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖŸàŸÇÿπ (ŸÖÿ´ŸÑÿßŸã: ŸÖÿ≠ŸÑ ÿ™ÿ¨ÿßÿ±Ÿä + ÿπŸÜŸàÿßŸÜ ÿ≥ŸÉŸÜ ÿßŸÑŸÖÿ™ŸáŸÖ)ÿå Ÿàÿ∂Ÿëÿ≠ ÿ∞ŸÑŸÉ ŸÅŸä ŸÜŸÇÿßÿ∑.\n\n"
        "## ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ ŸàÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑÿπŸäŸÜŸäÿ©\n\n"
        "- ÿ•ÿ∞ÿß ŸàŸèÿ¨ÿØÿ™ 'ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑÿπŸäŸÜŸäÿ©' ÿ£Ÿà ŸÖÿ≠ÿ∂ÿ± ŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ ÿ£Ÿà ÿ£ÿ¥Ÿäÿßÿ° ŸÖÿ™ŸÑŸÅÿ©/ŸÖÿ≥ÿ±ŸàŸÇÿ©ÿå "
        "ÿßŸÉÿ™ÿ®Ÿáÿß ŸÅŸä ÿ¥ŸÉŸÑ ŸÜŸÇÿßÿ∑ ŸÖÿ™ÿ™ÿßÿ®ÿπÿ©ÿå ŸÑŸÉŸÑ ÿπŸÜÿµÿ± ÿ≥ÿ∑ÿ± Ÿàÿßÿ≠ÿØ ŸÖÿ´ŸÑÿßŸã:\n"
        "- (ÿßŸÑÿØŸàÿ±: ŸÖÿ≥ÿ±ŸàŸÇ/ŸÖÿ™ŸÑŸÅ/ŸÖÿ≠ÿ¨Ÿàÿ≤...) ‚Äì (ÿßŸÑÿµŸÜŸÅ) ‚Äì (ÿßŸÑŸàÿµŸÅ ÿßŸÑÿ™ŸÅÿµŸäŸÑŸä) ‚Äì (ÿßŸÑÿ≠ÿßŸÑÿ©) ‚Äì (ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿ•ŸÜ Ÿàÿ¨ÿØÿ™).\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™Ÿàÿ¨ÿØ ŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÉÿ™ÿ®: ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ ÿ£Ÿà ÿ£ÿ¥Ÿäÿßÿ° ÿπŸäŸÜŸäÿ© ŸÖÿ∞ŸÉŸàÿ±ÿ©.\n\n"
        "## ÿßŸÑÿ£ÿ∂ÿ±ÿßÿ± (ÿ•ŸÜ Ÿàÿ¨ÿØÿ™)\n\n"
        "- ÿ•ÿ∞ÿß Ÿàÿ±ÿØÿ™ ÿ£ÿ∂ÿ±ÿßÿ± ŸÖÿßÿØŸäÿ© (ŸÖÿ´ŸÑ ÿ™ŸÑŸÅ ŸÅŸä ÿ£ŸÇŸÅÿßŸÑÿå ÿ≤ÿ¨ÿßÿ¨ÿå ÿ£ÿ®Ÿàÿßÿ®...)ÿå "
        "ÿßÿ∞ŸÉÿ±Ÿáÿß ÿ£Ÿäÿ∂ÿßŸã ŸÅŸä ÿ¥ŸÉŸÑ ŸÜŸÇÿßÿ∑ÿå ŸÉŸÑ ŸÜŸÇÿ∑ÿ© ÿ™Ÿàÿ∂ÿ≠: ÿßŸÑŸÖŸàŸÇÿπ ÿßŸÑŸÖÿ™ÿ∂ÿ±ÿ± ‚Äì ŸàÿµŸÅ ÿßŸÑÿ∂ÿ±ÿ± ‚Äì ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ™ŸÇÿØŸäÿ±Ÿäÿ© ÿ•ŸÜ Ÿàÿ¨ÿØÿ™.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™Ÿèÿ∞ŸÉÿ± ÿ£ÿ∂ÿ±ÿßÿ±ÿå ÿßŸÉÿ™ÿ®: ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ÿ∂ÿ±ÿßÿ± ŸÖÿ∞ŸÉŸàÿ±ÿ©.\n\n"
        "## ÿßŸÑÿ£ŸÇŸàÿßŸÑ\n\n"
        "ÿßŸÉÿ™ÿ® ÿ£ŸÇŸàÿßŸÑ ŸÉŸÑ ÿ∑ÿ±ŸÅ ŸÅŸä ŸÅŸÇÿ±ÿßÿ™ ŸÖŸÜŸÅÿµŸÑÿ©ÿå ŸÖÿ≥ÿ™ÿÆŸÑÿµÿ© ŸÖŸÜ ŸÖÿ≠ÿßÿ∂ÿ± ÿßŸÑÿßÿ≥ÿ™ÿ¨Ÿàÿßÿ®:\n\n"
        "**ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿ®ŸÑÿ∫/ÿßŸÑŸÖÿ™ÿ∂ÿ±ÿ±:**\n"
        "- ŸÑÿÆÿµ ŸÖÿß Ÿàÿ±ÿØ ŸÅŸä ŸÖÿ≠ÿßÿ∂ÿ± ÿ£ŸÇŸàÿßŸÑŸá (ÿ£ÿ≥ÿ¶ŸÑÿ©/ÿ£ÿ¨Ÿàÿ®ÿ©) ÿ®ÿØŸàŸÜ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿ®ÿ≠ÿ±ŸÅŸäÿ™Ÿáÿßÿå "
        "ŸÖÿπ ÿßŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑŸÖŸáŸÖÿ© (ŸÉŸäŸÅ ÿπÿ±ŸÅ ÿ®ÿßŸÑŸàÿßŸÇÿπÿ©ÿå ŸÖÿßÿ∞ÿß ÿ¥ÿßŸáÿØÿå ŸÖÿßÿ∞ÿß Ÿäÿ∑ŸÑÿ®...).\n\n"
        "**ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá:**\n"
        "- ŸÑÿÆÿµ ÿ•ŸÇÿ±ÿßÿ±Ÿá ÿ£Ÿà ÿ•ŸÜŸÉÿßÿ±Ÿá ŸÉŸÖÿß Ÿàÿ±ÿØ.\n\n"
        "**ÿ£ŸÇŸàÿßŸÑ ÿßŸÑÿ¥ŸáŸàÿØ (ÿ•ŸÜ Ÿàÿ¨ÿØŸàÿß):**\n"
        "- ŸÑŸÉŸÑ ÿ¥ÿßŸáÿØ ŸÅŸÇÿ±ÿ© ŸÇÿµŸäÿ±ÿ© ÿ™ŸÑÿÆÿµ ŸÖÿß ŸÇÿßŸÑŸá.\n\n"
        "## ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿ¥ÿ±ÿ∑ÿ©\n\n"
        "- **ŸáÿßŸÖ:** ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿ£Ÿä ÿ¨ÿØŸàŸÑ ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ.\n"
        "- ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÖŸÜ ÿ¨ŸÖŸäÿπ ŸÖÿ≠ÿßÿ∂ÿ± \"ŸÅÿ™ÿ≠ ÿßŸÑŸÖÿ≠ÿ∂ÿ±\" ŸàŸÖÿß ÿ®ÿπÿØŸáÿß ÿ™ÿ≥ŸÑÿ≥ŸÑ ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿ¥ÿ±ÿ∑ÿ© "
        "ŸÖÿ±ÿ™ÿ®ÿ© ÿ≤ŸÖŸÜŸäÿßŸã ŸÖŸÜ ÿßŸÑÿ£ŸÇÿØŸÖ ÿ•ŸÑŸâ ÿßŸÑÿ£ÿ≠ÿØÿ´ ŸÅŸä ÿ¥ŸÉŸÑ ŸÜŸÇÿßÿ∑:\n"
        "- ÿ™ŸÑŸÇŸä ÿßŸÑÿ®ŸÑÿßÿ∫ÿå ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿßŸÑÿ∂ÿßÿ®ÿ∑ ÿßŸÑŸÖŸÜÿßŸàÿ®ÿå ÿßŸÑÿßŸÜÿ™ŸÇÿßŸÑ ŸÑŸÖÿ≥ÿ±ÿ≠ ÿßŸÑÿ¨ÿ±ŸäŸÖÿ©ÿå ÿßŸÑŸÖÿπÿßŸäŸÜÿ© ŸàÿßŸÑÿ™ÿµŸàŸäÿ±ÿå "
        "ÿ™ŸÅÿ±Ÿäÿ∫ ŸÉÿßŸÖŸäÿ±ÿßÿ™ ÿßŸÑŸÖÿ±ÿßŸÇÿ®ÿ©ÿå ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ£ŸÖŸÜŸäÿ©ÿå ÿ•ÿ±ÿ≥ÿßŸÑ ÿßŸÑÿ•ÿ≠ÿ∂ÿßÿ±Ÿäÿßÿ™ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸäÿ©ÿå "
        "ÿßŸÑŸÇÿ®ÿ∂ ÿπŸÑŸâ ÿßŸÑŸÖÿ™ŸáŸÖÿå ÿ™ŸàŸÇŸäŸÅŸáÿå Ÿàÿ∫Ÿäÿ±Ÿáÿß.\n\n"
        "## ÿßŸÑÿ™ŸÜÿßÿ≤ŸÑ ÿ£Ÿà ÿßŸÑÿµŸÑÿ≠\n\n"
        "- ÿ•ÿ∞ÿß Ÿàÿ±ÿØ ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ± ÿ£Ÿà ŸÅŸä ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿ£ŸÜ ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ Ÿäÿ±ÿ∫ÿ®ŸàŸÜ ŸÅŸä ÿßŸÑÿµŸÑÿ≠ ÿ£Ÿà ŸÑÿß Ÿäÿ±ÿ∫ÿ®ŸàŸÜÿå "
        "ÿ£Ÿà ÿ£ŸÜ ŸáŸÜÿßŸÉ ÿ™ŸÜÿßÿ≤ŸÑ ÿπŸÜ ÿßŸÑÿ®ŸÑÿßÿ∫ÿå ÿßÿ∞ŸÉÿ± ÿ™ŸÅÿßÿµŸäŸÑ ÿ∞ŸÑŸÉ (ÿßŸÑÿ™ÿßÿ±ŸäÿÆÿå ŸÖŸÜ ÿ™ŸÜÿßÿ≤ŸÑÿå ŸáŸÑ ÿ™ŸÖ ÿØŸÅÿπ ŸÖÿ®ÿßŸÑÿ∫...).\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸèÿ∞ŸÉÿ± ÿ£Ÿä ÿ¥Ÿäÿ° ÿπŸÜ ÿßŸÑÿµŸÑÿ≠ ÿ£Ÿà ÿßŸÑÿ™ŸÜÿßÿ≤ŸÑÿå ÿßŸÉÿ™ÿ®: 'ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ'.\n\n"
        "## ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ŸàŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑŸÜŸäÿßÿ®ÿ©\n\n"
        "- ŸÑÿÆÿµ ŸÉÿßŸÅÿ© ŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿßŸÑÿπÿßŸÖÿ© ŸÉŸÖÿß Ÿàÿ±ÿØÿ™ ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ ŸÅŸä ÿ¥ŸÉŸÑ ŸÜŸÇÿßÿ∑:\n"
        "- ŸÖÿ´ŸÑ: ÿ∑ŸÑÿ® ÿßŸÑÿ•ÿ∞ŸÜ ÿ®ÿ∂ÿ®ÿ∑ Ÿàÿ•ÿ≠ÿ∂ÿßÿ± ÿßŸÑŸÖÿ™ŸáŸÖÿå ÿßŸÑÿ™ÿµÿ±ŸÅ ŸÅŸä ÿßŸÑŸÇÿ∂Ÿäÿ©ÿå ÿßŸÑÿ•ÿ≠ÿßŸÑÿ© ŸÑŸÜŸäÿßÿ®ÿ© ŸÖÿπŸäŸÜÿ©ÿå ÿßŸÑÿ≠ŸÅÿ∏ÿå "
        "ÿßŸÑÿ•ÿ≠ÿßŸÑÿ© ŸÑŸÑŸÖÿ≠ŸÉŸÖÿ©ÿå ÿ™ŸÉŸÑŸäŸÅ ÿ®ÿßŸÑÿ≠ÿ∂Ÿàÿ±ÿå ÿ•ÿµÿØÿßÿ± ŸÑÿßÿ¶ÿ≠ÿ© ÿßŸÑÿßÿ™ŸáÿßŸÖÿå ÿßŸÑÿ±ÿ®ÿ∑ ÿßŸÑÿ¢ŸÑŸä ŸÖÿπ ÿßŸÑŸÖÿ≠ÿßŸÉŸÖ...\n"
        "- ŸÑŸÉŸÑ ŸÇÿ±ÿßÿ±: ÿßÿ∞ŸÉÿ± ÿßŸÑÿ™ÿßÿ±ŸäÿÆ (ÿ•ŸÜ Ÿàÿ¨ÿØ)ÿå Ÿàÿßÿ≥ŸÖ ŸàŸÉŸäŸÑ ÿßŸÑŸÜŸäÿßÿ®ÿ© ÿ£Ÿà ÿπÿ∂Ÿà ÿßŸÑŸÜŸäÿßÿ®ÿ©ÿå ŸàŸÜÿµ ÿßŸÑŸÇÿ±ÿßÿ± ÿ®ÿßÿÆÿ™ÿµÿßÿ±.\n\n"
        "## ÿ™ÿ≥ŸÑŸäŸÖ ÿßŸÑŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ (ÿ•ŸÜ Ÿàÿ¨ÿØ)\n\n"
        "- ÿ•ÿ∞ÿß ÿ™ŸÖ ÿ™ÿ≥ŸÑŸäŸÖ ÿ£Ÿä ŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ ÿ£Ÿà ŸÖÿ®ÿßŸÑÿ∫ ŸÑÿ¥ÿÆÿµ ÿ£Ÿà ÿ¨Ÿáÿ©ÿå ÿßÿ∞ŸÉÿ± ŸÖŸÜ ÿßÿ≥ÿ™ŸÑŸÖÿå ŸÖÿ™Ÿâÿå ŸàŸÖÿß ŸáŸä ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑŸÖÿ≥ŸÑŸëŸéŸÖÿ© ŸÅŸä ŸÜŸÇÿßÿ∑.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸèÿ∞ŸÉÿ± ÿ™ÿ≥ŸÑŸäŸÖ ŸÖÿ∂ÿ®Ÿàÿ∑ÿßÿ™ÿå ÿßŸÉÿ™ÿ®: 'ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ'.\n\n"
        "## ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸáŸÖÿ© ŸàÿßŸÑÿÆÿ∑ ÿßŸÑÿ≤ŸÖŸÜŸä\n\n"
        "- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿ¨ÿØŸàŸÑ ŸáŸÜÿßÿõ ÿßŸÉÿ™ŸÅŸê ÿ®ŸÇÿßÿ¶ŸÖÿ© ŸÖÿ±ÿ™ÿ®ÿ© ÿ≤ŸÖŸÜŸäÿßŸã.\n"
        "- ÿßŸÉÿ™ÿ® ŸÇÿßÿ¶ŸÖÿ© ŸÖŸÜÿ≥ŸÇÿ© ÿ®ÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸàÿßŸÑÿ£ÿ≠ÿØÿßÿ´ ŸÖÿ´ŸÑ:\n"
        "  - ÿßŸÑÿ™ÿßÿ±ŸäÿÆ ŸàÿßŸÑŸàŸÇÿ™ ‚Äì ÿßŸÑÿ≠ÿØÿ´ ‚Äì ÿßŸÑÿ¨Ÿáÿ©/ÿßŸÑÿ¥ÿÆÿµ (ŸÖÿ´ÿßŸÑ: 2025-07-22 14:02 ‚Äì ÿ™ŸÑŸÇŸä ÿßŸÑÿ®ŸÑÿßÿ∫ ŸÅŸä ŸÖÿ±ŸÉÿ≤ ÿ¥ÿ±ÿ∑ÿ© ÿ¨ŸÜŸàÿ® ÿßŸÑÿπÿßÿµŸÖÿ© ‚Äì ŸÖÿ±ŸÉÿ≤ ÿßŸÑÿ¥ÿ±ÿ∑ÿ©).\n\n"
        "## ÿßŸÑÿ™ŸàŸÇŸäÿπÿßÿ™ ŸàÿßŸÑŸÖÿ≠ÿ±ÿ±ŸäŸÜ\n\n"
        "- ÿßÿ∞ŸÉÿ± ÿ£ÿ≥ŸÖÿßÿ° Ÿàÿ±ÿ™ÿ® ŸÖÿ≠ÿ±ÿ±Ÿä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ± (ŸÖÿ≥ÿßÿπÿØ ŸÖŸÑÿßÿ≤ŸÖÿå ÿπÿ±ŸäŸÅÿå ÿ±ŸÇŸäÿ® ÿ£ŸàŸÑ...)\n"
        "- ÿßÿ∞ŸÉÿ± ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑÿ∂ÿ®ÿßÿ∑ ÿßŸÑŸÖÿ¥ÿ±ŸÅŸäŸÜ (ŸÖÿ´ÿßŸÑ: ÿßŸÑÿ±ÿßÿ¶ÿØ/ ÿπŸÑŸä ŸÅÿßÿ∂ŸÑÿå ÿßŸÑŸÜŸÇŸäÿ®/ ÿ≠ÿ≥ŸäŸÜ ÿ¥ÿßŸÉÿ±).\n"
        "- ÿßÿ∞ŸÉÿ± ÿ£ÿ≥ŸÖÿßÿ° ÿ£ÿπÿ∂ÿßÿ° ÿßŸÑŸÜŸäÿßÿ®ÿ© Ÿàÿ£ÿÆÿµÿßÿ¶Ÿä ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ ŸÉŸÖÿß Ÿàÿ±ÿØÿ™.\n"
        "- ŸäŸÖŸÉŸÜŸÉ ŸÉÿ™ÿßÿ®ÿ© ÿ∞ŸÑŸÉ ŸÅŸä ŸÜŸÇÿßÿ∑ ÿ®ÿ≥Ÿäÿ∑ÿ©.\n\n"
        "## ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿÆÿ™ÿßŸÖŸäÿ© (ÿ•ŸÜ ŸàŸèÿ¨ÿØ ÿ¨ÿØŸàŸÑ ÿ£ÿ≥ÿ¶ŸÑÿ©)\n\n"
        "- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ÿ¨ÿØŸàŸÑ ŸÅŸä ÿ¢ÿÆÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ ÿ®ÿπŸÜŸàÿßŸÜ 'ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©' (ŸÖÿ´ŸÑ: ŸáŸÑ ŸäŸàÿ¨ÿØ ÿ™ÿµŸàŸäÿ± ÿ£ŸÖŸÜŸäÿü ŸáŸÑ Ÿäÿ±ÿ∫ÿ® ÿ£ÿ∑ÿ±ÿßŸÅ ÿßŸÑÿ®ŸÑÿßÿ∫ ŸÅŸä ÿßŸÑÿµŸÑÿ≠ÿü)ÿå\n"
        "  ŸÅÿ≠ŸàŸëŸêŸÑŸá ÿ•ŸÑŸâ ŸÜŸÇÿßÿ∑ ŸÜÿµŸäÿ© ŸÖÿ´ŸÑ:\n"
        "  - ÿßŸÑÿ≥ÿ§ÿßŸÑ: (ÿßŸÑŸÜÿµ ŸÉŸÖÿß Ÿàÿ±ÿØ) ‚Äì ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: (ŸÉŸÖÿß Ÿàÿ±ÿØÿ™).\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™Ÿàÿ¨ÿØ ÿ£ÿ≥ÿ¶ŸÑÿ© ÿÆÿ™ÿßŸÖŸäÿ©ÿå ŸäŸÖŸÉŸÜŸÉ ÿ≠ÿ∞ŸÅ Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ ÿ£Ÿà ŸÉÿ™ÿßÿ®ÿ©: ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ±.\n\n"
        "## ŸÖŸÑÿßÿ≠ŸÇ ÿ•ÿ∂ÿßŸÅŸäÿ© (ÿ•ŸÜ Ÿàÿ¨ÿØÿ™)\n\n"
        "- ÿ∂ÿπ ŸáŸÜÿß ÿ£Ÿä ŸÖÿ≠ÿ™ŸàŸâ ŸÖŸáŸÖ ŸÑŸÉŸÜŸá ŸÑŸäÿ≥ ÿ¨ÿ≤ÿ°ÿßŸã ŸÖÿ®ÿßÿ¥ÿ±ÿßŸã ŸÖŸÜ ÿ™ÿ≥ŸÑÿ≥ŸÑ ÿßŸÑŸÇÿ∂Ÿäÿ©ÿå "
        "ŸÖÿ´ŸÑ: ÿ®ŸÑÿßÿ∫ÿßÿ™ ÿ£ÿÆÿ±Ÿâ ŸÖÿπŸÑŸëŸéŸÇÿ© ÿπŸÑŸâ ŸÜŸÅÿ≥ ÿßŸÑÿ¥ÿÆÿµ ÿ®ÿØŸàŸÜ ÿ™ŸÅÿßÿµŸäŸÑÿå ÿ£Ÿà ŸÜŸÖÿßÿ∞ÿ¨ ÿ•ÿØÿßÿ±Ÿäÿ© ŸÖŸÉÿ±ÿ±ÿ©ÿå "
        "ÿ£Ÿà ÿµŸÅÿ≠ÿßÿ™ ŸÑÿß ÿ™ÿ∂ŸäŸÅ ÿ¨ÿØŸäÿØÿßŸã ŸÑŸÑŸÇÿ∂Ÿäÿ© ŸàŸÑŸÉŸÜ ŸÇÿØ ÿ™Ÿèÿ∞ŸÉÿ± ŸÉŸÖÿ±ÿ¨ÿπ.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™Ÿàÿ¨ÿØ ŸÖŸÑÿßÿ≠ŸÇ ÿ≠ŸÇŸäŸÇŸäÿ©ÿå ŸÑÿß ÿ™ŸÉÿ™ÿ® Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ.\n\n"
        "ÿ™ÿπŸÑŸäŸÖÿßÿ™ ŸÜŸáÿßÿ¶Ÿäÿ©:\n"
        "- ÿßŸÉÿ™ÿ® ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑ÿå ÿ®ÿßÿ™ÿ®ÿßÿπ ÿßŸÑŸáŸäŸÉŸÑ ÿßŸÑÿ≥ÿßÿ®ŸÇ ÿ®ÿßŸÑŸÉÿßŸÖŸÑ Ÿàÿ®ÿ™ÿ≥ŸÑÿ≥ŸÑ ÿßŸÑÿπŸÜÿßŸàŸäŸÜ.\n"
        "- ŸÑÿß ÿ™ŸÜÿ≥ÿÆ ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ© ÿ£Ÿà ÿ±ÿ§Ÿàÿ≥ ÿßŸÑÿµŸÅÿ≠ÿßÿ™ ÿ•ŸÑÿß ÿπŸÜÿØ ÿßŸÑÿ≠ÿßÿ¨ÿ©.\n"
        "- ŸÑÿß ÿ™ÿ™ÿ±ŸÉ ÿ£Ÿä ŸÇÿ≥ŸÖ ŸÅÿßÿ±ÿ∫ÿßŸã ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖÿ™ÿßÿ≠ÿ© ŸÅŸä ÿßŸÑŸÜÿµ.\n"
        "- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ™ŸàŸÅÿ± ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ŸÑŸÇÿ≥ŸÖ ŸÖÿπŸäŸÜÿå ÿßŸÉÿ™ÿ® ÿµÿ±ÿßÿ≠ÿ©: 'ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ'.\n"
        "- ÿ™ÿ∞ŸÉŸëÿ±: ÿßŸÑÿ¨ÿØŸàŸÑ ÿßŸÑŸàÿ≠ŸäÿØ ÿßŸÑŸÖÿ≥ŸÖŸàÿ≠ ÿ®Ÿá ŸÅŸä ÿßŸÑÿ•ÿÆÿ±ÿßÿ¨ ŸáŸà ÿ¨ÿØŸàŸÑ (ÿßŸÑÿ£ÿ∑ÿ±ÿßŸÅ)ÿå "
        "Ÿàÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑÿ£ÿÆÿ±Ÿâ Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ŸÜÿµÿßŸã ÿ£Ÿà ŸÜŸÇÿßÿ∑ÿßŸã ŸÅŸÇÿ∑.\n"
    )

    return system, user



def split_text_into_chunks(text: str) -> List[str]:
    """Split text into chunks with overlap to maintain context."""
    if len(text) <= CHUNK_SIZE:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + CHUNK_SIZE

        # If not the last chunk, try to break at sentence boundary
        if end < len(text):
            # Look for sentence endings within last 200 chars
            search_start = max(start, end - 200)
            last_period = text.rfind(".", search_start, end)
            last_newline = text.rfind("\n", search_start, end)
            break_point = max(last_period, last_newline)

            if break_point > start:
                end = break_point + 1

        chunks.append(text[start:end])
        start = end - OVERLAP_SIZE if end < len(text) else end

    logger.info(f"Split text into {len(chunks)} chunks")
    return chunks


def retry_with_simple_prompt(chunk_text: str, chunk_num: int, total_chunks: int) -> str:
    """
    Retry with a simpler, more neutral prompt to avoid content moderation.
    This is a fallback when the main prompt triggers safety filters.
    """
    logger.info(f"üîÑ Retrying chunk {chunk_num} with simplified prompt")
    
    simple_prompt = (
        "Ÿäÿ±ÿ¨Ÿâ ÿ™ŸÜÿ∏ŸäŸÖ Ÿàÿ™ÿ≠ÿ≥ŸäŸÜ ÿµŸäÿßÿ∫ÿ© ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÑŸä ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ. "
        "ÿßÿ≠ÿ™ŸÅÿ∏ ÿ®ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸàÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸàÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸÉŸÖÿß ŸáŸä ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ÿå ŸÅŸÇÿ∑ ÿ≠ÿ≥ŸëŸÜ ÿßŸÑÿµŸäÿßÿ∫ÿ© ŸàÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ.\n\n"
        f"{chunk_text}"
    )
    
    request_body = {
        "messages": [{"role": "user", "content": [{"text": simple_prompt}]}],
        "inferenceConfig": {
            "maxTokens": MAX_TOKENS,
            "temperature": 0.0,
            "topP": 0.8
        }
    }
    
    try:
        response = bedrock_runtime.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(request_body, ensure_ascii=False).encode("utf-8")
        )
        
        response_body = json.loads(response["body"].read())
        stop_reason = response_body.get("stopReason", "")
        
        if isinstance(stop_reason, str) and ("content_filtered" in stop_reason.lower() or "blocked" in stop_reason.lower()):
            logger.warning(f"‚ö†Ô∏è Content still filtered. Returning original text with note.")
            return f"[ŸÑÿß ŸäŸÖŸÉŸÜ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿµŸäÿßÿ∫ÿ© ÿ®ÿ≥ÿ®ÿ® ŸÅŸÑÿßÿ™ÿ± ÿßŸÑÿ£ŸÖÿßŸÜ]\n\n{chunk_text}"
        
        return response_body["output"]["message"]["content"][0]["text"]
        
    except Exception as e:
        logger.error(f"Retry failed for chunk {chunk_num}: {e}")
        # Last resort: return original with note
        return f"[ŸÑŸÖ ÿ™ÿ™ŸÖ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿµŸäÿßÿ∫ÿ© - ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ©]\n\n{chunk_text}"


def call_bedrock_for_chunk(chunk_text: str, chunk_num: int, total_chunks: int) -> str:
    """Call Bedrock to rewrite a single chunk."""
    system_prompt, _ = build_rewrite_prompts("")

    user_prompt = (
        f"ÿ£ÿπÿØ ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÜ ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ (ÿßŸÑÿ¨ÿ≤ÿ° {chunk_num} ŸÖŸÜ {total_chunks}). "
        "ÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ≠ŸÇÿßÿ¶ŸÇ ŸàÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸàÿßŸÑÿ™Ÿàÿßÿ±ŸäÿÆ ŸÉŸÖÿß ŸáŸä.\n\n"
        f"{chunk_text}\n\n"
        "ÿßŸÉÿ™ÿ® ÿßŸÑŸÜÿ≥ÿÆÿ© ÿßŸÑŸÖÿπÿßÿØ ÿµŸäÿßÿ∫ÿ™Ÿáÿß ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ŸÅŸÇÿ∑."
    )

    request_body = {
        "system": [{"text": system_prompt}],
        "messages": [{"role": "user", "content": [{"text": user_prompt}]}],
        "inferenceConfig": {
            "maxTokens": MAX_TOKENS,
            "temperature": 0.0,
            "topP": 0.8
        }
    }

    try:
        response = bedrock_runtime.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(request_body, ensure_ascii=False).encode("utf-8")
        )

        response_body = json.loads(response["body"].read())
        stop_reason = response_body.get("stopReason", "")

        if isinstance(stop_reason, str) and ("content_filtered" in stop_reason.lower() or "blocked" in stop_reason.lower()):
            logger.warning(f"‚ö†Ô∏è Content filtered by Bedrock. Trying alternative approach...")
            # Retry with simplified prompt
            return retry_with_simple_prompt(chunk_text, chunk_num, total_chunks)

        return response_body["output"]["message"]["content"][0]["text"]

    except Exception as e:
        error_str = str(e)
        logger.error(f"Chunk {chunk_num} failed: {error_str}")
        
        # Check if it's a content moderation error
        if "ValidationException" in error_str or "throttling" in error_str.lower():
            logger.warning(f"‚ö†Ô∏è Bedrock error (possibly content moderation). Trying alternative approach...")
            return retry_with_simple_prompt(chunk_text, chunk_num, total_chunks)
        
        raise


def call_bedrock_for_rewrite(original_text: str) -> str:
    """Rewrite document by processing in chunks if needed."""
    chunks = split_text_into_chunks(original_text)

    if len(chunks) == 1:
        logger.info("Processing as single chunk")
        return call_bedrock_for_chunk(chunks[0], 1, 1)

    logger.info(f"Processing {len(chunks)} chunks")
    rewritten_chunks = []

    for i, chunk in enumerate(chunks, 1):
        logger.info(f"Processing chunk {i}/{len(chunks)}")
        rewritten = call_bedrock_for_chunk(chunk, i, len(chunks))
        rewritten_chunks.append(rewritten)

    # Merge chunks, removing overlap duplicates
    result = rewritten_chunks[0]
    for chunk in rewritten_chunks[1:]:
        # Simple merge - append with newline
        result += "\n" + chunk

    logger.info(f"Merged {len(chunks)} chunks. Final length: {len(result)}")
    return result


# ---------------------- Entity Extraction & Validation ----------------------

ROLE_KEYWORDS = [
    "ŸÖÿ®ŸÑÿ∫", "ŸÖÿØÿπŸâ ÿπŸÑŸäŸá", "ŸÖÿ¥ŸÉŸàŸãŸâ ÿπŸÑŸäŸá", "ŸÖÿ¥ŸÉŸà ÿπŸÑŸäŸá", "ÿ¥ÿßŸáÿØ", "ŸÖÿ¥ÿ™ÿ®Ÿá", "ŸÖÿ∞ŸÉŸàÿ±",
    "ŸÖÿ¨ŸÜŸä ÿπŸÑŸäŸá", "ŸÖÿ™ŸáŸÖ", "ŸÖÿ≠ÿßŸÖŸä", "ŸÇÿßÿ∂Ÿä", "ŸÖÿ≠ŸÇŸÇ", "ŸÉÿßÿ™ÿ® ÿ∂ÿ®ÿ∑"
]

SECTION_KEYWORDS = [
    "ŸÖŸÇÿØŸÖÿ©", "ŸÖŸÑÿÆÿµ", "ŸÖŸÑÿÆÿµ ÿßŸÑÿ≠ÿßÿØÿ´", "ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑŸàÿßŸÇÿπÿ©", "ŸàŸÇÿßÿ¶ÿπ ÿßŸÑŸÇÿ∂Ÿäÿ©", "ÿ£ŸÇŸàÿßŸÑ",
    "ÿßŸÑÿ¥ŸáŸàÿØ", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑÿ¥ÿßŸÉŸä", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿØÿπŸâ ÿπŸÑŸäŸá", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑŸÖÿ®ŸÑÿ∫", "ÿ£ŸÇŸàÿßŸÑ ÿßŸÑÿ¥ÿßŸáÿØ",
    "ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ", "ÿßŸÑÿ™ŸÇÿ±Ÿäÿ±", "ÿßŸÑÿÆÿßÿ™ŸÖÿ©", "ÿßŸÑÿ∑ŸÑÿ®ÿßÿ™", "ÿßŸÑŸÇÿ±ÿßÿ±", "ÿ£ÿ∑ÿ±ÿßŸÅ ÿßŸÑÿ®ŸÑÿßÿ∫", "ŸÖÿ≥ÿ±ÿ≠ ÿßŸÑÿ≠ÿßÿØÿ´"
]


def extract_entities(text: str) -> Dict[str, Set[str]]:
    """Extract names, roles, case numbers, dates, times, IDs, locations from Arabic text."""
    names: Set[str] = set()

    # Naive Arabic name pattern (2-5 tokens of letters) ‚Äì conservative to reduce false positives
    for m in re.finditer(r"\b[\u0621-\u064A]{2,}(?:\s+[\u0621-\u064A]{2,}){1,4}\b", text):
        nm = m.group(0).strip()
        excluded_terms = (
            r"\b(ŸÖŸÖŸÑŸÉÿ©|Ÿàÿ≤ÿßÿ±ÿ©|ÿßŸÑŸÜŸäÿßÿ®ÿ©|ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜ|ÿ¥ÿ±ÿ∑ÿ©|ŸÇÿ±ÿßÿ±|ÿ®ŸÑÿßÿ∫|ÿßŸÑŸÇÿ∂Ÿäÿ©|ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ|ÿßŸÑŸÖÿ≠ŸÉŸÖÿ©|"
            r"ÿßŸÑÿ¨ŸÜÿßÿ¶Ÿäÿ©|ÿßŸÑÿπÿßŸÖÿ©|ÿßŸÑÿ£ŸÖŸÜ|ÿßŸÑÿπÿØŸÑ|ÿßŸÑŸÇÿßŸÜŸàŸÜ|ÿßŸÑÿ≠ŸÉŸàŸÖÿ©|ÿßŸÑÿØÿßÿÆŸÑŸäÿ©|ŸÜŸäÿßÿ®ÿ©|ŸÖÿ±ŸÉÿ≤ ÿ¥ÿ±ÿ∑ÿ©)\b"
        )
        if len(nm.split()) >= 2 and not re.search(excluded_terms, nm):
            names.add(nm)

    roles: Set[str] = set()
    for kw in ROLE_KEYWORDS:
        if re.search(rf"\b{re.escape(kw)}\b", text):
            roles.add(kw)

    # Case number patterns
    case_numbers: Set[str] = set(
        re.findall(r"(?:ÿ±ŸÇŸÖ\s*(?:ÿßŸÑÿ®ŸÑÿßÿ∫|ÿßŸÑŸÇÿ∂Ÿäÿ©)\s*[:Ôºö]?\s*(\d{2,}))", text)
    )

    # Dates: dd/mm/yyyy or dd-mm-yyyy or yyyy-mm-dd
    dates: Set[str] = set(
        re.findall(r"\b(?:\d{1,2}[\-/]\d{1,2}[\-/]\d{2,4}|\d{4}[\-/]\d{1,2}[\-/]\d{1,2})\b", text)
    )

    # Times: HH:MM(:SS)
    times: Set[str] = set(re.findall(r"\b\d{1,2}:\d{2}(?::\d{2})?\b", text))

    # National IDs: 9-12 digits
    national_ids: Set[str] = set(re.findall(r"\b\d{9,12}\b", text))

    # Locations: very crude detection via ÿ®ÿπÿØ 'ŸÅŸä'/'ÿ®ŸÄ'
    locations: Set[str] = set()
    for m in re.finditer(
        r"\b(?:ŸÅŸä|ÿ®ŸÄ)\s+([\u0621-\u064A]{2,}(?:\s+[\u0621-\u064A]{2,}){0,3})\b", text
    ):
        loc = m.group(1).strip()
        if not re.search(r"\b(ÿßŸÑŸÖÿ∞ŸÉŸàÿ±|ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ©|ÿßŸÑŸÖÿØÿπŸâ|ÿßŸÑÿ¥ÿßŸÉŸä|ÿßŸÑŸÖÿ™ŸáŸÖ)\b", loc):
            locations.add(loc)

    sections: Set[str] = set()
    for kw in SECTION_KEYWORDS:
        # Allow heading followed by colon and either space or newline
        pattern = rf"\n\s*{re.escape(kw)}\s*[:Ôºö]?(?:\s|\n)"
        if re.search(pattern, text):
            sections.add(kw)

    return {
        "names": names,
        "roles": roles,
        "case_numbers": case_numbers,
        "dates": dates,
        "times": times,
        "national_ids": national_ids,
        "locations": locations,
        "sections": sections
    }


def remove_duplicated_paragraphs(text: str) -> str:
    """Remove exact duplicate paragraphs that often appear due to artifacts."""
    paras = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]
    seen: Set[str] = set()
    result: List[str] = []

    for p in paras:
        key = hashlib.md5(p.encode("utf-8")).hexdigest()
        if key not in seen:
            seen.add(key)
            result.append(p)

    return "\n\n".join(result)


def case_boundary_protection(text: str) -> str:
    """
    Light cleanup to remove obviously fabricated headings but do NOT truncate
    valid multi-page reports or repeated headers like ÿ±ŸÇŸÖ ÿßŸÑÿ®ŸÑÿßÿ∫.
    """
    forbidden_heads = [
        r"^\s*ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑÿ±ÿ≥ŸÖŸä\s*$",
        r"^\s*ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ\s*[:Ôºö]?\s*$"
    ]

    lines = text.splitlines()
    cleaned_lines: List[str] = []

    for ln in lines:
        if any(re.match(pat, ln.strip()) for pat in forbidden_heads):
            continue
        cleaned_lines.append(ln)

    return "\n".join(cleaned_lines)


def validate_and_sanitize(original: str, rewritten: str) -> Tuple[bool, str, List[str]]:
    """
    Validate rewritten text against original entities and structure.
    Returns (is_valid, sanitized_text, violations).
    If invalid, sanitized_text is cleaned.
    """
    orig = extract_entities(original)
    new = extract_entities(rewritten)
    violations: List[str] = []

    # New names introduced
    extra_names = new["names"] - orig["names"]
    if extra_names:
        violations.append(f"ÿ£ÿ≥ŸÖÿßÿ° ÿ¨ÿØŸäÿØÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ£ÿµŸÑ: {', '.join(sorted(extra_names))}")

    # New roles introduced
    extra_roles = new["roles"] - orig["roles"]
    if extra_roles:
        violations.append(f"ÿ£ÿØŸàÿßÿ± ÿ¨ÿØŸäÿØÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ£ÿµŸÑ: {', '.join(sorted(extra_roles))}")

    # Case number duplication/new
    if len(new["case_numbers"]) > len(orig["case_numbers"]) or (
        new["case_numbers"] - orig["case_numbers"]
    ):
        violations.append("ÿ±ŸÇŸÖ ÿ®ŸÑÿßÿ∫/ŸÇÿ∂Ÿäÿ© ÿ•ÿ∂ÿßŸÅŸä ÿ£Ÿà ŸÖÿÆÿ™ŸÑŸÅ ÿ™ŸÖ ÿ•ÿØÿÆÿßŸÑŸá")

    # Missing critical info: names or case numbers removed
    missing_names = orig["names"] - new["names"]
    if missing_names:
        violations.append(
            f"ÿ™ŸÖ ÿ≠ÿ∞ŸÅ ÿ®ÿπÿ∂ ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸÖŸÜ ÿßŸÑŸÜÿµ: {', '.join(sorted(missing_names))}"
        )

    missing_case_numbers = orig["case_numbers"] - new["case_numbers"]
    if missing_case_numbers:
        violations.append("ÿ™ŸÖ ÿ≠ÿ∞ŸÅ ÿ±ŸÇŸÖ ÿ®ŸÑÿßÿ∫/ŸÇÿ∂Ÿäÿ© ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä")

    # Sections fabricated
    fabricated_sections = new["sections"] - orig["sections"]
    if fabricated_sections:
        violations.append(
            f"ÿ£ŸÇÿ≥ÿßŸÖ ŸÖŸèÿ∂ÿßŸÅÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ£ÿµŸÑ: {', '.join(sorted(fabricated_sections))}"
        )

    # Detect duplicated paragraphs
    para_counts: Dict[str, int] = {}
    for p in re.split(r"\n{2,}", rewritten):
        p = p.strip()
        if not p:
            continue
        para_counts[p] = para_counts.get(p, 0) + 1

    if any(c > 1 for c in para_counts.values()):
        violations.append("ÿ™ŸÉÿ±ÿßÿ± ŸÅŸÇÿ±ÿßÿ™")

    # Sanitization steps: remove duplicates, run light boundary protection
    sanitized = remove_duplicated_paragraphs(rewritten)
    sanitized = case_boundary_protection(sanitized)

    # If violations exist, try removing some obviously fabricated lines
    if violations:
        lines = sanitized.splitlines()
        kept: List[str] = []
        for ln in lines:
            if re.search(r"\b(?:ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑÿ±ÿ≥ŸÖŸä|ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿ≠ŸÇŸäŸÇ)\b", ln):
                continue
            kept.append(ln)
        sanitized = "\n".join(kept)

    is_valid = len(violations) == 0
    return is_valid, sanitized, violations


def update_job_status(job_id: str, status: str, data: Optional[Dict] = None) -> None:
    """Update job status in S3."""
    status_key = f"rewrite-jobs/{job_id}/status.json"
    
    status_data = {
        "jobId": job_id,
        "status": status,
        "updatedAt": datetime.utcnow().isoformat()
    }
    
    if data:
        status_data.update(data)
    
    s3_client.put_object(
        Bucket=BUCKET_NAME,
        Key=status_key,
        Body=json.dumps(status_data, ensure_ascii=False),
        ContentType="application/json"
    )
    
    logger.info(f"Updated job {job_id} status to {status}")


def save_rewritten_result(job_id: str, session_id: str, rewritten_text: str, original_length: int) -> str:
    """Save the rewritten text to S3 and return the key."""
    # Save to rewritten/{sessionId}.txt (single latest per session)
    result_key = f"rewritten/{session_id}.txt"
    
    s3_client.put_object(
        Bucket=BUCKET_NAME,
        Key=result_key,
        Body=rewritten_text.encode("utf-8"),
        ContentType="text/plain; charset=utf-8",
        Metadata={
            "jobId": job_id,
            "sessionId": session_id,
            "originalLength": str(original_length),
            "rewrittenLength": str(len(rewritten_text))
        }
    )
    
    logger.info(f"Saved rewritten result for job {job_id} to {result_key}")
    return result_key


def lambda_handler(event: Dict, context: Any) -> None:
    """
    Worker Lambda handler - processes rewrite jobs asynchronously.
    Does not return a response to API Gateway.
    """
    job_id = None
    
    try:
        # Extract job details from event (sent by Lambda 1)
        job_id = event.get("jobId")
        text = event.get("text")
        s3_key = event.get("s3Key")
        session_id = event.get("sessionId", "unknown")
        
        if not job_id:
            logger.error("No job ID provided in event")
            return
        
        logger.info(f"üîÑ Processing rewrite job {job_id} for session {session_id}")
        logger.info(f"üìã Event details: text={bool(text)}, s3_key={s3_key}")
        
        # Get input text
        if not text:
            if s3_key:
                # Check if s3_key is a folder or a file
                actual_key = s3_key
                
                # If it looks like a folder, find the text file inside
                if not s3_key.endswith('.txt'):
                    logger.info(f"üìÅ s3_key appears to be a folder: '{s3_key}'")
                    
                    # First try to find .txt file in the specified folder
                    found_key = find_text_file_in_folder(BUCKET_NAME, s3_key)
                    
                    # If no .txt file found in specified folder, try the latest folder
                    if not found_key:
                        logger.warning(f"‚ö†Ô∏è No .txt file in specified folder, searching latest folder...")
                        latest_folder = find_latest_extracted_folder(BUCKET_NAME)
                        
                        if latest_folder:
                            logger.info(f"üîÑ Trying latest folder: {latest_folder}")
                            found_key = find_text_file_in_folder(BUCKET_NAME, latest_folder)
                    
                    if not found_key:
                        logger.error(f"‚ùå No .txt file found in folder: {s3_key} or latest folder")
                        update_job_status(job_id, "FAILED", {
                            "error": f"No .txt file found. Please ensure text has been extracted from the document.",
                            "sessionId": session_id,
                            "searchedPath": s3_key
                        })
                        return
                    
                    actual_key = found_key
                    logger.info(f"‚úÖ Using file: {actual_key}")
                
                text = read_text_from_s3(BUCKET_NAME, actual_key)
                logger.info(f"‚û° Loaded text from S3: {get_safe_log_info(text, session_id)}")
            else:
                logger.error(f"No text or s3Key provided for job {job_id}")
                update_job_status(job_id, "FAILED", {
                    "error": "No text or s3Key provided",
                    "sessionId": session_id
                })
                return
        
        # Validate text size
        if len(text) > MAX_TOTAL_CHARS:
            logger.warning(f"Job {job_id}: Text too long ({len(text)} chars)")
            update_job_status(job_id, "FAILED", {
                "error": f"Text too long. Max {MAX_TOTAL_CHARS} chars allowed",
                "currentChars": len(text),
                "sessionId": session_id
            })
            return
        
        # Perform rewrite
        logger.info(f"Starting Bedrock processing for job {job_id}")
        bedrock_output = call_bedrock_for_rewrite(text)
        
        # Validate and sanitize
        is_valid, sanitized, violations = validate_and_sanitize(text, bedrock_output)
        
        if not is_valid:
            logger.warning(f"Job {job_id}: Validation violations: {violations}")
            rewritten_text = sanitized
        else:
            rewritten_text = bedrock_output
        
        # Save result to S3
        result_key = save_rewritten_result(job_id, session_id, rewritten_text, len(text))
        
        # Update status to COMPLETED
        update_job_status(job_id, "COMPLETED", {
            "resultKey": result_key,
            "resultLength": len(rewritten_text),
            "originalLength": len(text),
            "model": MODEL_ID,
            "sessionId": session_id,
            "validationPassed": is_valid,
            "violations": violations if not is_valid else []
        })
        
        logger.info(f"‚úÖ Job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Error processing job {job_id}: {e}", exc_info=True)
        
        if job_id:
            update_job_status(job_id, "FAILED", {
                "error": str(e),
                "errorType": type(e).__name__
            })
